{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain langchain-community dataset sentence_transformers langchain-chroma langchain-huggingface\n!pip install transformers huggingface_hub rank_bm25","metadata":{"execution":{"iopub.status.busy":"2024-10-26T21:08:07.119122Z","iopub.execute_input":"2024-10-26T21:08:07.119966Z","iopub.status.idle":"2024-10-26T21:09:05.292302Z","shell.execute_reply.started":"2024-10-26T21:08:07.119923Z","shell.execute_reply":"2024-10-26T21:09:05.291387Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\nCollecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\nCollecting langchain-chroma\n  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\nCollecting langchain-huggingface\n  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain)\n  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.13.3)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nCollecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n  Downloading chromadb-0.5.15-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: fastapi<1,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from langchain-chroma) (0.111.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.20.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (4.12.2)\nCollecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nCollecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.30.1)\nCollecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nCollecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.0)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.62.2)\nCollecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.3)\nCollecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.4)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.27.0)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.4)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (2.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence_transformers)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi<1,>=0.95.2->langchain-chroma) (2.6.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi<1,>=0.95.2->langchain-chroma) (2.1.5)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.30.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.3)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\nRequirement already satisfied: importlib-metadata<=7.1,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.0.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.63.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nRequirement already satisfied: opentelemetry-proto==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nCollecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (70.0.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.46b0)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (12.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.19.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.0)\nDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\nDownloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\nDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading chromadb-0.5.15-py3-none-any.whl (607 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.137-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\nDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\nDownloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\nDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=e9772f048ccf4e279228935f825ba94acf5ce3e6c11583cd95e802ac14bbcded\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, durationpy, banal, SQLAlchemy, pyproject_hooks, packaging, opentelemetry-util-http, mmh3, humanfriendly, chroma-hnswlib, bcrypt, backoff, asgiref, requests-toolbelt, posthog, coloredlogs, build, pydantic-settings, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, dataset, opentelemetry-instrumentation-asgi, langchain-core, sentence_transformers, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-huggingface, langchain, chromadb, langchain-community, langchain-chroma\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 2.0.30\n    Uninstalling SQLAlchemy-2.0.30:\n      Successfully uninstalled SQLAlchemy-2.0.30\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-1.4.54 asgiref-3.8.1 backoff-2.2.1 banal-1.0.6 bcrypt-4.2.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.15 coloredlogs-15.0.1 dataset-1.6.2 durationpy-0.9 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.4 langchain-chroma-0.1.4 langchain-community-0.3.3 langchain-core-0.3.13 langchain-huggingface-0.1.0 langchain-text-splitters-0.3.0 langsmith-0.1.137 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-util-http-0.46b0 packaging-24.1 posthog-3.7.0 pydantic-settings-2.6.0 pypika-0.48.9 pyproject_hooks-1.2.0 requests-toolbelt-1.0.0 sentence_transformers-3.2.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nCollecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank_bm25\nSuccessfully installed rank_bm25-0.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T21:09:09.229101Z","iopub.execute_input":"2024-10-26T21:09:09.229909Z","iopub.status.idle":"2024-10-26T21:09:09.771888Z","shell.execute_reply.started":"2024-10-26T21:09:09.229866Z","shell.execute_reply":"2024-10-26T21:09:09.770977Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from IPython.display import display\nimport logging\nclass DisplayHandler(logging.Handler):\n    def emit(self, record):\n        display(self.format(record))\n\n# Configure logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Add custom display handler\ndisplay_handler = DisplayHandler()\ndisplay_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s:%(message)s'))\nlogger.addHandler(display_handler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T21:09:12.128861Z","iopub.execute_input":"2024-10-26T21:09:12.129789Z","iopub.status.idle":"2024-10-26T21:09:12.135786Z","shell.execute_reply.started":"2024-10-26T21:09:12.129744Z","shell.execute_reply":"2024-10-26T21:09:12.134747Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from langchain.document_loaders import CSVLoader\nfrom datasets import load_dataset, concatenate_datasets, Value\nfrom huggingface_hub import hf_hub_download\nimport ast\n\nfile_path1 = hf_hub_download(repo_id=\"niloufarna/MovieChat\", subfolder='imdb', filename=\"imdb_cornell_movie_details_dataset.csv\", local_dir= '.', repo_type=\"dataset\")\nfile_path2 = hf_hub_download(repo_id=\"niloufarna/MovieChat\", subfolder='imdb', filename=\"imdb_movieqa_details_dataset.csv\", local_dir ='.', repo_type=\"dataset\")\n\n\ndataset1 = load_dataset('csv', data_files = file_path1, split = 'train' )\ndataset2 = load_dataset('csv', data_files = file_path2, split = 'train' )\n\n# Remove rows where 'synopsis', 'plot', and 'plot_outline' are all NaN (None in Hugging Face)\nfiltered_dataset1 = dataset1.filter(\n    lambda row: (\n        row['plot_outline'] is not None and row['movie_name'] != 'Cherry Falls'\n))\n\n# Fill NaN values in 'synopsis' with the value from 'plot_outline'\ndef fill_movie_details(example):\n    if example['synopsis'] is None and example['plot_outline'] is not None:\n        example['synopsis'] = example['plot_outline']\n    if example['movie_name'] == 'spare me':\n        example['votes'] = 31\n        example['rating'] = 6.8\n    return example\n\ndef updata_dataset2(example):\n   if example['movie_name'] == 'Men in Black 3: Gag Reel':\n       example['plot_outline'] = \"The Men in Black 3: Gag Reel is a behind-the-scenes compilation of bloopers and funny moments during the filming of Men in Black 3. It features actors like Will Smith and Tommy Lee Jones breaking character, laughing through mistakes, and showcasing some lighthearted moments that occur during production. These reels are typically included as part of bonus material in the movie's DVD/Blu-ray release, offering fans a glimpse of the fun and camaraderie on set.\"\n   \n   return example\n\n# Apply the transformation function to the dataset\ndataset1 = filtered_dataset1.map(fill_movie_details)\n# Modify the 'votes' feature type to int64\ndataset1 = dataset1.cast_column('votes', Value('int64'))\n\n\n# Define a function to count null values (None) in each row\ndef count_nulls(example):\n    return {'null_count': sum(1 for value in example.values() if value is None)}\n\n# Apply the function to calculate null counts for each row\nnull_count_dataset = dataset1.map(count_nulls)\n\n# # Sum all null counts to get the total number of null values in the dataset\ntotal_nulls = sum(null_count_dataset['null_count'])\n\n# Print the total number of null values\nprint(f\"Total number of null values in the dataset: {total_nulls}\")\n\ndataset2 = dataset2.map(updata_dataset2)\n\ncombined_dataset = concatenate_datasets([dataset1, dataset2])\ndef convert_list_string(example):\n    # Parse the string representation of a list into an actual list\n   genres_list = ast.literal_eval(example['genres'])  # replace 'your_column_name' with the actual column name\n   languages_list = ast.literal_eval(example['languages'])\n   director_list = ast.literal_eval(example['director'])\n   writer_list = ast.literal_eval(example['writer'])\n   cast_list = ast.literal_eval(example['cast'])\n   character_names_list = ast.literal_eval(example['character_names'])\n   # Join the list elements into a single string\n   example['genres'] = ', '.join(genres_list)\n   example['languages'] = ', '.join(languages_list)\n   example['director'] = ', '.join(genres_list)\n   example['writer'] = ', '.join(languages_list)\n   example['cast'] = ', '.join(genres_list)\n   example['character_names'] = ', '.join(languages_list)\n\ncombined_dataset.map(convert_list_string)\n\npath = 'imdb_movie_dataset.csv'\ncombined_dataset.to_csv(path)\n\n\n# # Load CSV data from URLs\n# loader = CSVLoader(path)\n\n# documents = loader.load()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:54.669745Z","iopub.execute_input":"2024-10-26T20:47:54.670235Z","iopub.status.idle":"2024-10-26T20:47:59.746046Z","shell.execute_reply.started":"2024-10-26T20:47:54.670190Z","shell.execute_reply":"2024-10-26T20:47:59.745127Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:47:55,802:INFO:NumExpr defaulting to 4 threads.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:47:56,019:INFO:PyTorch version 2.4.0 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:47:56,023:INFO:Polars version 1.9.0 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:47:56,026:INFO:TensorFlow version 2.16.1 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:47:56,030:INFO:JAX version 0.4.26 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)b/imdb_cornell_movie_details_dataset.csv:   0%|          | 0.00/6.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7554f1143cc743eaa2f50fab9d25649b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"imdb/imdb_movieqa_details_dataset.csv:   0%|          | 0.00/5.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157a301ad18a44e5a479dca09bbd7672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f443ed8a429402eb030667c452a6c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df78da5028f54d08948b921afe6abdf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/615 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"745339a681d043929bb531591b10c617"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5dea2cf56334dcd8fb4d5d356a75936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffda60705e647a594d45f04b6afc94b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9366ed79a9b4d8ba5113b13b3549d98"}},"metadata":{}},{"name":"stdout","text":"Total number of null values in the dataset: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93df878d52224156a72dd52b9718c626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aaeddc91942425c973ac4af61ec1ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e135bf5bb46433fa0e9d80d02026a85"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"12462581"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# !pip install redis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:59.747173Z","iopub.execute_input":"2024-10-26T20:47:59.747468Z","iopub.status.idle":"2024-10-26T20:47:59.752213Z","shell.execute_reply.started":"2024-10-26T20:47:59.747435Z","shell.execute_reply":"2024-10-26T20:47:59.750543Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# import redis\n# import json\n\n# # Create a Redis connection\n\n\n# r = redis.Redis(\n#   host='redis-11169.c16.us-east-1-3.ec2.redns.redis-cloud.com',\n#   port=11169,\n#   password='vrntyCBNWv3k1133D7P3a79oIdYvLsIa')\n\n# # Define a function to store metadata in Redis\n# def store_metadata_in_redis(dataset):\n#     for row in dataset:\n#         # Create a unique key using movie_name and year\n#         key = f\"{row['movie_name']}:{row['year']}\"\n        \n#         # Store the metadata (excluding the synopsis/plot for now)\n#         metadata = {\n#             'rating': row.get('rating'),\n#             'votes': row.get('votes'),\n#             'genre': row.get('genre'),\n#             'director': row.get('director'),\n#             'writer': row.get('writer'),\n#             'cast': row.get('cast'),\n#             'character_names':row.get('character_names'),\n#             'languages': row.get('languages'),\n#             'countries':row.get('countries')\n#         }\n\n#         # Store the data in Redis as a JSON object\n#         r.set(key, json.dumps(metadata))\n\n# # Apply the function to the documents (this assumes `documents` is a list of dictionaries)\n# store_metadata_in_redis(combined_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:59.753374Z","iopub.execute_input":"2024-10-26T20:47:59.753633Z","iopub.status.idle":"2024-10-26T20:47:59.809857Z","shell.execute_reply.started":"2024-10-26T20:47:59.753604Z","shell.execute_reply":"2024-10-26T20:47:59.809041Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# # Test connection with Redis\n# try:\n#     r.ping()\n#     print(\"Connected to Redis!\")\n# except redis.ConnectionError as e:\n#     print(f\"Failed to connect to Redis: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:59.810883Z","iopub.execute_input":"2024-10-26T20:47:59.811197Z","iopub.status.idle":"2024-10-26T20:47:59.822856Z","shell.execute_reply.started":"2024-10-26T20:47:59.811165Z","shell.execute_reply":"2024-10-26T20:47:59.822023Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# def retrieve_metadata_from_redis(movie_name, year):\n#     key = f\"{movie_name}:{year}\"\n    \n#     # Retrieve the data from Redis\n#     metadata = r.get(key)\n    \n#     # If the key exists, return the metadata as a Python dictionary\n#     if metadata:\n#         return json.loads(metadata)\n#     else:\n#         print(f\"No data found for key: {key}\")\n#         return None\n\n# # Example of retrieving metadata for a movie\n# movie_name = 'a nightmare on elm street 4: the dream master'\n# year = 1988\n# metadata = retrieve_metadata_from_redis(movie_name, year)\n\n# if metadata:\n#     print(f\"Metadata for {movie_name} ({year}):\", metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:59.823907Z","iopub.execute_input":"2024-10-26T20:47:59.824216Z","iopub.status.idle":"2024-10-26T20:47:59.833019Z","shell.execute_reply.started":"2024-10-26T20:47:59.824185Z","shell.execute_reply":"2024-10-26T20:47:59.832154Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n# from langchain_chroma import Chroma\n# from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n# # Initialize embeddings and Chroma\n# device = 'cuda' if SentenceTransformer('all-MiniLM-L6-v2').device.type == 'cuda' else 'cpu'\n# model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n# chroma_index = Chroma(embedding_function=embedding_function,collection_name=\"movie_collection\", persist_directory=\"./chroma_data\")\n\n# # Define a text splitter\n# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n\n# # Concatenate plot, plot_outline, and synopsis into a single string\n# # documents = [\n# #     f\"Movie: {row['movie_name']} ({row['year']})\\nPlot: {row['plot']}\\nPlot Outline: {row['plot_outline']}\\nSynopsis: {row['synopsis']}\"\n# #     for row in combined_dataset\n# # ]\n\n\n\n# print(\"before chromadb\")\n# # Split the concatenated documents into chunks and embed them\n# # Define a batch size for processing\n# BATCH_SIZE = 50\n\n# # Placeholder for accumulating chunks and metadata in batches\n# batch_chunks = []\n# batch_metadata = []\n\n# for i,row in enumerate(combined_dataset):\n#     # Log each movie processed\n#     logger.debug(f\"Processing movie: {row['movie_name']}\")\n\n#     full_text = (\n#         f\"Movie Name: {row['movie_name']}\\n\"\n#         f\"Year: {row['year']}\\n\"\n#         f\"Rating: {row.get('rating')}\\n\"\n#         f\"Votes: {row.get('votes')}\\n\"\n#         f\"Genre: {row.get('genre')}\\n\"\n#         f\"Director: {row.get('director')}\\n\"\n#         f\"Writer: {row.get('writer')}\\n\"\n#         f\"Cast: {row.get('cast')}\\n\"\n#         f\"Character Names: {row.get('character_names')}\\n\"\n#         f\"Languages: {row.get('languages')}\\n\"\n#         f\"Countries: {row.get('countries')}\\n\"\n#         f\"Plot: {row['plot']}\\n\"\n#         f\"Plot Outline: {row['plot_outline']}\\n\"\n#         f\"Synopsis: {row['synopsis']}\"\n#     )\n\n#     metadata = {'movie_name': row['movie_name'], 'year': row['year']}\n#     chunks = text_splitter.split_text(full_text)\n    \n#     batch_chunks.extend(chunks)\n#     batch_metadata.extend([metadata] * len(chunks))\n\n#     # When batch size is reached, add to Chroma\n#     if len(batch_chunks) >= BATCH_SIZE:\n#         logger.info(f\"Adding batch of {len(batch_chunks)} chunks to Chroma.\")\n#         chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n#         chroma_index.persist()\n#         batch_chunks, batch_metadata = [], []  # Clear the batch lists\n\n# # Add remaining chunks in the last batch\n# if batch_chunks:\n#     logger.info(f\"Adding final batch of {len(batch_chunks)} chunks to Chroma.\")\n#     chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n#     chroma_index.persist()\n\n\n# logger.info(\"Embedding and adding texts to Chroma completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:59.834378Z","iopub.execute_input":"2024-10-26T20:47:59.834698Z","iopub.status.idle":"2024-10-26T20:47:59.845564Z","shell.execute_reply.started":"2024-10-26T20:47:59.834665Z","shell.execute_reply":"2024-10-26T20:47:59.844686Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nfrom langchain_chroma import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport multiprocessing as mp\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nimport torch\n\nBATCH_SIZE = 50\n\n# Initialize the embedding model with GPU support if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nembedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": device})\n\n\n# Initialize Chroma vector store\nchroma_index = Chroma(\n    embedding_function=embedding_function,\n    collection_name=\"movie_collection\",\n    persist_directory=\"./chroma_data\"\n)\n\n# Define a text splitter with optimized chunk size and overlap\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n\n# Function to process each movie\ndef process_movie(row):\n    full_text = (\n        f\"Movie Name: {row['movie_name']}\\n\"\n        f\"Year: {row['year']}\\n\"\n        f\"Rating: {row.get('rating')}\\n\"\n        f\"Votes: {row.get('votes')}\\n\"\n        f\"Genre: {row.get('genre')}\\n\"\n        f\"Director: {row.get('director')}\\n\"\n        f\"Writer: {row.get('writer')}\\n\"\n        f\"Cast: {row.get('cast')}\\n\"\n        f\"Character Names: {row.get('character_names')}\\n\"\n        f\"Languages: {row.get('languages')}\\n\"\n        f\"Countries: {row.get('countries')}\\n\"\n        f\"Plot: {row['plot']}\\n\"\n        f\"Plot Outline: {row['plot_outline']}\\n\"\n        f\"Synopsis: {row['synopsis']}\"\n    )\n    metadata = {\n        'movie_name': row['movie_name'],\n        'year': row['year']\n    }\n    chunks = text_splitter.split_text(full_text)\n    return chunks, [metadata] * len(chunks)\n\nlogger.info(\"Starting ChromaDB integration...\")\n\n# Initialize multiprocessing pool\npool = mp.Pool(mp.cpu_count())\nlogger.info(f\"Number of CPUs: {mp.cpu_count()}\")\n# Process movies in parallel\nresults = pool.map(process_movie, combined_dataset)\npool.close()\npool.join()\nlogger.info(f\"Size of results {len(results)}\")\n\n# Initialize batch lists\nbatch_chunks = []\nbatch_metadata = []\n\n# Iterate through processed results\nfor chunks, metadatas in results:\n    batch_chunks.extend(chunks)\n    batch_metadata.extend(metadatas)\n    \n    if len(batch_chunks) >= BATCH_SIZE:\n        logger.info(f\"Adding batch of {len(batch_chunks)} chunks to Chroma.\")\n        chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n        batch_chunks, batch_metadata = [], []  # Clear the batch lists\n\n# Add remaining chunks\nif batch_chunks:\n    logger.info(f\"Adding final batch of {len(batch_chunks)} chunks to Chroma.\")\n    chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n    \n\nlogger.info(\"Embedding and adding texts to Chroma completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:47:59.849635Z","iopub.execute_input":"2024-10-26T20:47:59.850028Z","iopub.status.idle":"2024-10-26T20:49:36.351634Z","shell.execute_reply.started":"2024-10-26T20:47:59.849966Z","shell.execute_reply":"2024-10-26T20:49:36.350584Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:18,778:INFO:Load pretrained SentenceTransformer: all-MiniLM-L6-v2'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b055276535c431384851aaf8ff8af5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047cc64ddca74b3c9a694f78f1427f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b01d9272cd0400b88c4565b45a2eff7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9920fe96c85240b88193457b456f5cc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f2cab040b64b50a0fb2664747eed0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2369133f672d4411b0a51a0f9a95053f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf8f155ff73941f58048f29ade9c7e39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fc3a76c0e594391a9063dd92602615e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab13409c6e9c4698830ba9c0f6271c03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba834b2d2f25409da42ecd2e9f6d3f7d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d876463e63f640e38a3371b0334c509c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:21,063:INFO:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:21,640:INFO:Starting ChromaDB integration...'"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:21,707:INFO:Number of CPUs: 4'"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:23,651:INFO:Size of results 1002'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:23,653:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:24,328:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:24,531:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:24,783:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:24,979:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:25,191:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:25,404:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:25,717:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:25,970:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:26,180:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:26,413:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:26,609:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:26,802:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:27,014:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:27,209:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:27,454:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:27,652:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:27,844:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:28,086:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:28,296:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:28,474:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:28,745:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:29,035:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:29,232:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:29,479:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:29,748:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:29,970:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:30,165:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:30,413:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:30,974:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:31,202:INFO:Adding batch of 71 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:31,454:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:31,645:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:31,863:INFO:Adding batch of 87 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:32,191:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:32,460:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:32,667:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:32,866:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:33,102:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:33,382:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:33,626:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:33,823:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:34,023:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:34,254:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:34,465:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:34,658:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:34,894:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:35,170:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:35,392:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:35,585:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:35,800:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:36,003:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:36,297:INFO:Adding batch of 76 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:36,587:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:36,798:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:36,996:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:37,191:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:37,471:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:37,754:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:38,008:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:38,240:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:38,478:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:38,713:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:38,938:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:39,197:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:39,419:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:39,679:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:39,865:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:40,100:INFO:Adding batch of 88 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:40,488:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:40,732:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:40,946:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:41,165:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:41,354:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:41,583:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:41,805:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:42,036:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:42,342:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:42,652:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:42,895:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:43,196:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:43,457:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:43,783:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:43,964:INFO:Adding batch of 88 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:44,300:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:44,636:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:44,831:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:45,046:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:45,281:INFO:Adding batch of 71 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:45,537:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:45,781:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:46,028:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:46,298:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:46,508:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:46,764:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:47,061:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:47,297:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:47,491:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:47,739:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:47,979:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:48,228:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:48,474:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:48,785:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:49,004:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:49,284:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:49,510:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:49,754:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:49,974:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:50,220:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:50,460:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:50,694:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:50,936:INFO:Adding batch of 77 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:51,237:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:51,486:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:51,790:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:52,038:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:52,251:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:52,534:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:52,892:INFO:Adding batch of 76 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:53,186:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:53,383:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:53,623:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:53,904:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:54,127:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:54,394:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:54,621:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:54,835:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:55,514:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:55,745:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:56,006:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:56,228:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:56,497:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:56,776:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:57,047:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:57,252:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:57,596:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:57,838:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:58,085:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:58,266:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:58,506:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:58,741:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:58,949:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:59,143:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:59,369:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:59,629:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:48:59,870:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:00,091:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:00,308:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:00,560:INFO:Adding batch of 78 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:00,911:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:01,149:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:01,591:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:01,923:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:02,279:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:02,531:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:02,778:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:03,002:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:03,219:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:03,458:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:03,679:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:03,895:INFO:Adding batch of 87 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:04,227:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:04,460:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:04,724:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:04,949:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:05,206:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:05,453:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:05,729:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:06,149:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:06,377:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:06,600:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:06,877:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:07,110:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:07,321:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:07,535:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:07,763:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:07,995:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:08,207:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:08,427:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:08,638:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:08,868:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:09,132:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:09,357:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:09,580:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:09,803:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:10,145:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:10,425:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:10,716:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:10,956:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:11,156:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:11,389:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:11,607:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:11,840:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:12,100:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:12,333:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:12,559:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:12,757:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:13,003:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:13,202:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:13,444:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:13,679:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:13,924:INFO:Adding batch of 81 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:14,326:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:14,515:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:14,791:INFO:Adding batch of 84 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:15,161:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:15,389:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:15,628:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:15,839:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:16,113:INFO:Adding batch of 75 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:16,390:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:16,632:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:16,873:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:17,103:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:17,354:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:17,611:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:17,861:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:18,078:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:18,393:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:18,598:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:18,861:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:19,080:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:19,409:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:19,738:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:20,004:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:20,326:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:20,629:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:20,896:INFO:Adding batch of 101 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:21,271:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:21,499:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:21,729:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:21,956:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:22,179:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:22,397:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:22,630:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:22,962:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:23,238:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:23,455:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:23,680:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:23,909:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:24,168:INFO:Adding batch of 87 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:24,513:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:24,721:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:24,989:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:25,213:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:25,462:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:25,676:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:25,908:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:26,209:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:26,462:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:26,694:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:27,014:INFO:Adding batch of 83 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:27,322:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:27,583:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:27,858:INFO:Adding batch of 84 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:28,179:INFO:Adding batch of 92 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:28,545:INFO:Adding batch of 98 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:28,967:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:29,207:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:29,474:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:29,715:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:29,931:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:30,148:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:30,385:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:30,645:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:30,900:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:31,234:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:31,426:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:31,651:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:31,937:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:32,138:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:32,380:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:32,618:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:32,829:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:33,116:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:33,377:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:33,623:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:33,865:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:34,078:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:34,315:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:34,549:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:34,758:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:34,997:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:35,316:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:35,562:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:35,774:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:36,004:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:36,251:INFO:Adding final batch of 24 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 20:49:36,347:INFO:Embedding and adding texts to Chroma completed.'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from langchain.retrievers import EnsembleRetriever, BM25Retriever\nretriever_vanilla = chroma_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\nretriever_mmr = chroma_index.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\nretriever_BM25 = BM25Retriever.from_texts(chunks, search_kwargs={\"k\": 4})\n# initialize the ensemble retriever with 3 Retrievers\nensemble_retriever = EnsembleRetriever(\n    retrievers=[retriever_vanilla, retriever_mmr, retriever_BM25], weights=[0.3, 0.3, 0.4]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:36.353045Z","iopub.execute_input":"2024-10-26T20:49:36.353727Z","iopub.status.idle":"2024-10-26T20:49:36.527646Z","shell.execute_reply.started":"2024-10-26T20:49:36.353692Z","shell.execute_reply":"2024-10-26T20:49:36.526858Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEndpoint\nfrom langchain.memory import ConversationSummaryMemory\n\n# Hugging Face Hub integration\nfrom transformers import AutoModelForCausalLM\n\n# Define the model ID\nmodel_id = \"NousResearch/Llama-2-7b-chat-hf\"\n\n\n# Load the model (this might take some time depending on your connection)\nllm = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\n\n# Initialize the Hugging Face Hub LLM for the BART model\nsummary_llm = HuggingFaceEndpoint(\n    repo_id=\"facebook/bart-large-cnn\",  # BART model for summarization\n   temperature= 0.7  # Adjust parameters if needed\n)\n\n# Initialize the conversation memory using the summarization model\nmemory = ConversationSummaryMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"response\", llm=summary_llm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T21:09:40.929156Z","iopub.execute_input":"2024-10-26T21:09:40.929539Z","iopub.status.idle":"2024-10-26T21:13:16.289901Z","shell.execute_reply.started":"2024-10-26T21:09:40.929506Z","shell.execute_reply":"2024-10-26T21:13:16.287502Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564385ce49a54397b1303372559a887c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c2241df7f642d1a3ef6674c0787213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7222492cdba47959d9a550394b46a81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d40e0bbfcf4e25bb0469cb024f9b30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc427ccfd6304880b9b6ca9d5a00e283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 21:12:21,089:INFO:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5f323579374725939def5f9a82b50d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16301bf3064c43baa5fb71f65e4ed96c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-26 21:13:16,123:WARNING:Some parameters are on the meta device because they were offloaded to the cpu.'"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/214036820.py:22: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationSummaryMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"response\", llm=summary_llm)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# from langchain.prompts import ChatPromptTemplate\n\n# # Define a prompt template for conversational tasks\n# template = \"\"\"\n# You are a helpful assistant. Please answer the following question:\n\n# Question: {question}\n# \"\"\"\n# #  \"\"\"Instruction:{}\n# # Input:{}\n# # Context:{}\n# # \"\"\"\n# prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.060059Z","iopub.execute_input":"2024-10-26T20:49:37.060721Z","iopub.status.idle":"2024-10-26T20:49:37.065067Z","shell.execute_reply.started":"2024-10-26T20:49:37.060673Z","shell.execute_reply":"2024-10-26T20:49:37.064033Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # Define the human message prompt template\n# human_message_template = PromptTemplate.from_template(\n#     \"\"\"\n#     You are an experienced assistant specializing in question-answering tasks. \n#     Utilize the provided context to respond to the question. \n#     If the answer is unknown, always state 'I don't know.' \n#     Never provide an answer you are unsure about and ensure it is concise.\n#     Your answer must be comprehensive and contain all of the relevant details in the Context.\n#     \\nQuestion: {question} \\nContext: {context} \\nResponse:\n#     \"\"\"\n# )\n\n# # Create a HumanMessagePromptTemplate instance using the defined prompt template\n# human_message_prompt_template = HumanMessagePromptTemplate(prompt=human_message_template)\n\n# # Create the ChatPromptTemplate with the input variables and messages, excluding metadata\n# chat_prompt_template = ChatPromptTemplate(\n#     input_variables=['context', 'input'],\n#     messages=[human_message_prompt_template]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.066254Z","iopub.execute_input":"2024-10-26T20:49:37.066559Z","iopub.status.idle":"2024-10-26T20:49:37.074766Z","shell.execute_reply.started":"2024-10-26T20:49:37.066527Z","shell.execute_reply":"2024-10-26T20:49:37.073971Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# from langchain.prompts import PromptTemplate\n# from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n# # Define the prompt template with concise instructions\n# # Define the system message prompt template\n# system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n#     \"\"\"You are an experienced assistant specializing in question-answering tasks.\n# Use the context to answer the following question.\n# If you're unsure, say 'I don't know.'\n# \\nInstruction: {instruction}\\n\n#     \"\"\")\n# # Define the human message prompt for the user's question\n# human_message_prompt_template = HumanMessagePromptTemplate.from_template(\"Question: {question}\")\n\n# prompt_template = PromptTemplate.from_template(\n# \"\"\"Chat history: {chat_history}\\nContext: {context}\n#     \"\"\"\n# )\n\n\n# chat_prompt_template = ChatPromptTemplate(\n#     input_variables=['instruction', 'question'],\n#     messages=[system_message_prompt_template,\n#             message_prompt_template,\n#             # human_message_prompt_template\n#              ]\n# )\n\n# chat_prompt_template2 = (\n#     +prompt_template\n#     + \"\\nResponse:\"\n# )\n\n# prompt = PipelinePromptTemplate(\n#     final_prompt=PromptTemplate.from_template(\"{context_prompt_template}\"),\n#     pipeline_prompts=[\n#         ('context_prompt_template', context_prompt_template)  # This will generate 'context_text' variable\n#         ('chat_prompt_template':)\n#     ]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.075763Z","iopub.execute_input":"2024-10-26T20:49:37.076041Z","iopub.status.idle":"2024-10-26T20:49:37.086810Z","shell.execute_reply.started":"2024-10-26T20:49:37.076006Z","shell.execute_reply":"2024-10-26T20:49:37.086114Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# chat_prompt_template = PromptTemplate.from_template(\n#     \"\"\"You are an experienced assistant specializing in question-answering tasks.\n# Use the context to answer the following question.\n# If you're unsure, say 'I don't know.'\n# \\nInstruction: {instruction}\\nChat history: {chat_history}\\nContext: {context}Question: {question}\\nResponse:\"\n#     \"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.087961Z","iopub.execute_input":"2024-10-26T20:49:37.088520Z","iopub.status.idle":"2024-10-26T20:49:37.099757Z","shell.execute_reply.started":"2024-10-26T20:49:37.088487Z","shell.execute_reply":"2024-10-26T20:49:37.098935Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# prompt = (\n#     PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n#     + \", make it funny\"\n#     + \"\\n\\nand in {language}\"\n# )\n\n\n# prompt.format(topic=\"sports\", language=\"Afrikaans\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.100746Z","iopub.execute_input":"2024-10-26T20:49:37.101057Z","iopub.status.idle":"2024-10-26T20:49:37.109221Z","shell.execute_reply.started":"2024-10-26T20:49:37.101021Z","shell.execute_reply":"2024-10-26T20:49:37.108406Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# formatted_prompt = chat_prompt_template.format(\n#     instruction=\"Please provide the answer based on the given context.\",\n#     question=\"What is the capital of France?\",\n#     chat_history=\"User: What is the largest continent? Assistant: Asia\",\n#     context=\"France is a country in Europe. The capital of France is Paris.\"\n# )\n\n# print(formatted_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.110286Z","iopub.execute_input":"2024-10-26T20:49:37.110586Z","iopub.status.idle":"2024-10-26T20:49:37.120955Z","shell.execute_reply.started":"2024-10-26T20:49:37.110553Z","shell.execute_reply":"2024-10-26T20:49:37.120151Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n# Define the prompt template with concise instructions\n# Define the system message prompt template\nsystem_message_prompt_template = SystemMessagePromptTemplate.from_template(\n    \"\"\"You are an experienced assistant specializing in question-answering tasks.\nUse the context to answer the following question.\nIf you're unsure, say 'I don't know.'\n\\nInstruction: {instruction}\\n\n    \"\"\")\n# Define the human message prompt for the user's question\nhuman_message_prompt_template = HumanMessagePromptTemplate.from_template(\"Question: {question}\")\n\ncontext_message_prompt_template = AIMessagePromptTemplate.from_template(\n\"\"\"Chat history: {chat_history}\\nContext: {context}\n    \"\"\"\n)\n\n\nchat_prompt_template = (ChatPromptTemplate(\n    input_variables=['instruction', 'question','chat_history','context'],\n    messages=[system_message_prompt_template,\n            context_message_prompt_template,\n            human_message_prompt_template\n             ]\n)\n+\"\\nResponse:\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.122264Z","iopub.execute_input":"2024-10-26T20:49:37.122840Z","iopub.status.idle":"2024-10-26T20:49:37.131311Z","shell.execute_reply.started":"2024-10-26T20:49:37.122797Z","shell.execute_reply":"2024-10-26T20:49:37.130428Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # Example question to pass to the chain\n# question_data = {\n#     \"instruction\": \"Answer the following question:\",\n#     \"question\": \"How does Frodo come to own Bilbo's Ring?\",\n#     \"context\": \"\",  # Placeholder context if no documents are retrieved\n#     \"chat_history\": \"\"  # Placeholder chat history if no history is available\n# }\n\nchat_prompt_template.pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:49:37.132326Z","iopub.execute_input":"2024-10-26T20:49:37.132602Z","iopub.status.idle":"2024-10-26T20:49:37.140814Z","shell.execute_reply.started":"2024-10-26T20:49:37.132570Z","shell.execute_reply":"2024-10-26T20:49:37.139972Z"}},"outputs":[{"name":"stdout","text":"================================\u001b[1m System Message \u001b[0m================================\n\nYou are an experienced assistant specializing in question-answering tasks.\nUse the context to answer the following question.\nIf you're unsure, say 'I don't know.'\n\nInstruction: \u001b[33;1m\u001b[1;3m{instruction}\u001b[0m\n\n    \n\n==================================\u001b[1m AI Message \u001b[0m==================================\n\nChat history: \u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\nContext: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n    \n\n================================\u001b[1m Human Message \u001b[0m=================================\n\nQuestion: \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n\n================================\u001b[1m Human Message \u001b[0m=================================\n\n\nResponse:\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\n\n# First, load the memory to access chat history\nloaded_memory = RunnablePassthrough.assign(\n    chat_history=RunnableLambda(memory.load_memory_variables) | RunnableLambda(itemgetter(\"chat_history\"))\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:54:12.439926Z","iopub.execute_input":"2024-10-26T20:54:12.440333Z","iopub.status.idle":"2024-10-26T20:54:12.445637Z","shell.execute_reply.started":"2024-10-26T20:54:12.440297Z","shell.execute_reply":"2024-10-26T20:54:12.444647Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from langchain_core.output_parsers.string import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n# Function to format documents\n\ndef format_docs(docs):\n    # Ensure docs is a list of Document objects with a 'page_content' attribute\n    return \"\\n\".join([d.page_content for d in docs if hasattr(d, \"page_content\")])\n\n# Chain definition\nconversation_chain = (\n    RunnableParallel({\n        \"instruction\": RunnableLambda(lambda inputs: inputs[\"instruction\"]),\n        \"context\": RunnableLambda(lambda inputs: inputs[\"question\"])  # Extract \"question\" key for the retriever\n        | ensemble_retriever | RunnableLambda(format_docs),  # Retrieve and format documents as context\n        \"question\": RunnableLambda(lambda inputs: inputs[\"question\"]),   # Pass the user's question through\n        \"chat_history\": loaded_memory\n    })\n    # The dictionary ends here, and chaining follows\n    | chat_prompt_template  # Use a prompt to handle the user question\n    | llm  # Model to generate the answer\n    | StrOutputParser()  # Parse the model's output as a string\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:56:31.538356Z","iopub.execute_input":"2024-10-26T20:56:31.539256Z","iopub.status.idle":"2024-10-26T20:56:31.546448Z","shell.execute_reply.started":"2024-10-26T20:56:31.539214Z","shell.execute_reply":"2024-10-26T20:56:31.545535Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Example question to pass to the chain\nquestion_data = {\n    \"instruction\": \"Answer the following question:\",\n    \"question\": \"How does Frodo come to own Bilbo's Ring?\",\n    # \"context\": \"\",  # Placeholder context if no documents are retrieved\n    # \"chat_history\": \"\"  # Placeholder chat history if no history is available\n}\n\n# Run the chain with the provided question data\nresponse = conversation_chain.invoke(question_data)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-26T20:56:34.482661Z","iopub.execute_input":"2024-10-26T20:56:34.483815Z","iopub.status.idle":"2024-10-26T20:56:35.983807Z","shell.execute_reply.started":"2024-10-26T20:56:34.483758Z","shell.execute_reply":"2024-10-26T20:56:35.982031Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/NousResearch/Llama-2-7b-chat-hf","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m question_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer the following question:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does Frodo come to own Bilbo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Ring?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# \"context\": \"\",  # Placeholder context if no documents are retrieved\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# \"chat_history\": \"\"  # Placeholder chat history if no history is available\u001b[39;00m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the chain with the provided question data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconversation_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    402\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[1;32m    949\u001b[0m     ]\n\u001b[0;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 779\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1502\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1501\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1502\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1504\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1505\u001b[0m     )\n\u001b[1;32m   1506\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:288\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: e6g7nSEOxX8qQz8Qn3Wds)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/NousResearch/Llama-2-7b-chat-hf.\nMake sure your token has the correct permissions.\nThe model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."],"ename":"HfHubHTTPError","evalue":"(Request ID: e6g7nSEOxX8qQz8Qn3Wds)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/NousResearch/Llama-2-7b-chat-hf.\nMake sure your token has the correct permissions.\nThe model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"# from langchain.chains.combine_documents import StuffDocumentsChain\n# # Define document combination chain (e.g., StuffDocumentsChain)\n# combine_docs_chain = StuffDocumentsChain(llm=llm, prompt=chat_prompt_template)\n\n# # Define a question generator (rephrases question based on chat history)\n# question_generator = StuffDocumentsChain(llm=llm, prompt=chat_prompt_template)\n\n# # Initialize the ConversationalRetrievalChain\n# conversation_chain = ConversationalRetrievalChain(\n#     retriever=ensemble_retriever,\n#     combine_docs_chain=combine_docs_chain,\n#     question_generator=question_generator,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:19:05.456783Z","iopub.status.idle":"2024-10-25T23:19:05.457289Z","shell.execute_reply.started":"2024-10-25T23:19:05.457019Z","shell.execute_reply":"2024-10-25T23:19:05.457047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.chains.combine_documents import create_stuff_documents_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:19:05.458587Z","iopub.status.idle":"2024-10-25T23:19:05.459248Z","shell.execute_reply.started":"2024-10-25T23:19:05.458977Z","shell.execute_reply":"2024-10-25T23:19:05.459003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.chains import ConversationalRetrievalChain\n# # Initialize the conversational retrieval chain\n# conversation_chain = ConversationalRetrievalChain.from_llm(\n#     retriever=ensemble_retriever,  \n#     llm=llm, \n#     memory=memory,\n#     combine_docs_chain_kwargs={'prompt': chat_prompt_template},\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:26:27.205503Z","iopub.execute_input":"2024-10-25T23:26:27.205873Z","iopub.status.idle":"2024-10-25T23:26:27.211767Z","shell.execute_reply.started":"2024-10-25T23:26:27.205837Z","shell.execute_reply":"2024-10-25T23:26:27.210875Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from langchain.chains import (\n                create_history_aware_retriever,\n                create_retrieval_chain,\n            )\nhistory_aware_retriever = create_history_aware_retriever(\n                llm, ensemble_retriever, chat_prompt_template\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.chains import ConversationalRetrievalChain\n# from langchain.chains.combine_documents import StuffDocumentsChain\n\n# # Initialize a document combination chain with your prompt template\n# combine_docs_chain = StuffDocumentsChain(llm=llm, prompt=chat_prompt_template)\n\n# # Initialize the conversational retrieval chain using `combine_docs_chain`\n# conversation_chain = ConversationalRetrievalChain.from_llm(\n#     retriever=ensemble_retriever,\n#     llm=llm,\n#     memory=memory,\n#     combine_docs_chain=combine_docs_chain  # Use the combine_docs_chain directly\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:33:30.665637Z","iopub.execute_input":"2024-10-25T23:33:30.666554Z","iopub.status.idle":"2024-10-25T23:33:30.715120Z","shell.execute_reply.started":"2024-10-25T23:33:30.666511Z","shell.execute_reply":"2024-10-25T23:33:30.714062Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StuffDocumentsChain\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize a document combination chain with your prompt template\u001b[39;00m\n\u001b[1;32m      5\u001b[0m combine_docs_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mchat_prompt_template)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'StuffDocumentsChain' from 'langchain.chains.combine_documents' (/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'StuffDocumentsChain' from 'langchain.chains.combine_documents' (/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/__init__.py)","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"from langchain.chains import (\n                create_history_aware_retriever,\n                create_retrieval_chain,\n            )\n            from langchain.chains.combine_documents import create_stuff_documents_chain\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n            from langchain_openai import ChatOpenAI\n\n\n            retriever = ...  # Your retriever\n\n            llm = ChatOpenAI()\n\n            # Contextualize question\n            contextualize_q_system_prompt = (\n                \"Given a chat history and the latest user question \"\n                \"which might reference context in the chat history, \"\n                \"formulate a standalone question which can be understood \"\n                \"without the chat history. Do NOT answer the question, just \"\n                \"reformulate it if needed and otherwise return it as is.\"\n            )\n            contextualize_q_prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", contextualize_q_system_prompt),\n                    MessagesPlaceholder(\"chat_history\"),\n                    (\"human\", \"{input}\"),\n                ]\n            )\n            history_aware_retriever = create_history_aware_retriever(\n                llm, retriever, contextualize_q_prompt\n            )\n\n            # Answer question\n            qa_system_prompt = (\n                \"You are an assistant for question-answering tasks. Use \"\n                \"the following pieces of retrieved context to answer the \"\n                \"question. If you don't know the answer, just say that you \"\n                \"don't know. Use three sentences maximum and keep the answer \"\n                \"concise.\"\n                \"\\n\\n\"\n                \"{context}\"\n            )\n            qa_prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", qa_system_prompt),\n                    MessagesPlaceholder(\"chat_history\"),\n                    (\"human\", \"{input}\"),\n                ]\n            )\n            # Below we use create_stuff_documents_chain to feed all retrieved context\n            # into the LLM. Note that we can also use StuffDocumentsChain and other\n            # instances of BaseCombineDocumentsChain.\n            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n            rag_chain = create_retrieval_chain(\n                history_aware_retriever, question_answer_chain\n            )\n\n            # Usage:\n            chat_history = []  # Collect chat history here (a sequence of messages)\n            rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n\n    This chain takes in chat history (a list of messages) and new questions,\n    and then returns an answer to that question.\n    The algorithm for this chain consists of three parts:\n\n    1. Use the chat history and the new question to create a \"standalone question\".\n    This is done so that this question can be passed into the retrieval step to fetch\n    relevant documents. If only the new question was passed in, then relevant context\n    may be lacking. If the whole conversation was passed into retrieval, there may\n    be unnecessary information there that would distract from retrieval.\n\n    2. This new question is passed to the retriever and relevant documents are\n    returned.\n\n    3. The retrieved documents are passed to an LLM along with either the new question\n    (default behavior) or the original question and chat history to generate a final\n    response.\n\n    Example:\n        .. code-block:: python\n\n            from langchain.chains import (\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n            )\n            from langchain_core.prompts import PromptTemplate\n            from langchain_community.llms import OpenAI\n\n            combine_docs_chain = StuffDocumentsChain(...)\n            vectorstore = ...\n            retriever = vectorstore.as_retriever()\n\n            # This controls how the standalone question is generated.\n            # Should take `chat_history` and `question` as input variables.\n            template = (\n                \"Combine the chat history and follow up question into \"\n                \"a standalone question. Chat History: {chat_history}\"\n                \"Follow up question: {question}\"\n            )\n            prompt = PromptTemplate.from_template(template)\n            llm = OpenAI()\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n            chain = ConversationalRetrievalChain(\n                combine_docs_chain=combine_docs_chain,\n                retriever=retriever,\n                question_generator=question_generator_chain,\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain import LLMChain\n\n# llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the entire dataset\ntest_file_path = hf_hub_download(repo_id=\"niloufarna/MovieChat\", subfolder='dataset', filename=\"combined_movie_dataset.json\", local_dir= '.', repo_type=\"dataset\")\ndataset = load_dataset('json', data_files = test_file_path, split = 'train')\n# Filter each split based on the 'split' field\n\ntest_dataset = dataset['train'].filter(lambda x: x['split'] == 'test')\n\n\n# Example usage\n\nprint(\"Test Dataset:\", test_dataset)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generation = rag_chain.invoke({\"context\": ensemble_relevant_docs, \"question\": question})\nprint(generation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nresponse = llm_chain.run(question)\n\nprint(response)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset1[7]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"Find a movie similar to Inception\"\nquery_embedding = embedding_function.embed_query(query)\nsearch_results = chroma_index.similarity_search(query_embedding)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"documents","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset1.features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset2.features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset2[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf1 = pd.read_csv(file_path1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df1 = df1.dropna(subset=['synopsis','plot','plot_outline'], how='all')\ndf1['synopsis'] = df1['synopsis'].fillna(df1['plot_outline'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = pd.read_csv(file_path2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2[df2['plot_outline'].isna()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1[df1['votes'].isna()]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain langchain-community dataset sentence_transformers langchain-chroma langchain-huggingface\n!pip install transformers huggingface_hub rank_bm25 \n!pip install bitsandbytes>=0.39.0","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:51:48.761790Z","iopub.execute_input":"2024-10-28T17:51:48.762160Z","iopub.status.idle":"2024-10-28T17:53:01.339049Z","shell.execute_reply.started":"2024-10-28T17:51:48.762124Z","shell.execute_reply":"2024-10-28T17:53:01.337834Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\nCollecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\nCollecting langchain-chroma\n  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\nCollecting langchain-huggingface\n  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain)\n  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.13.3)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nCollecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n  Downloading chromadb-0.5.15-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: fastapi<1,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from langchain-chroma) (0.111.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.20.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (4.12.2)\nCollecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nCollecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.30.1)\nCollecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nCollecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.0)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.62.2)\nCollecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.3)\nCollecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.4)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.27.0)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.4)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (2.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence_transformers)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi<1,>=0.95.2->langchain-chroma) (2.6.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi<1,>=0.95.2->langchain-chroma) (2.1.5)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.30.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.3)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\nRequirement already satisfied: importlib-metadata<=7.1,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.0.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.63.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nRequirement already satisfied: opentelemetry-proto==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.25.0)\nCollecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (70.0.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.46b0)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (12.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.19.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.0)\nDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\nDownloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\nDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading chromadb-0.5.15-py3-none-any.whl (607 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.137-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\nDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\nDownloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\nDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=0307057af2bf9b2069144de3c49f7421ee64d1df1df8dcdffb58defc4d850e27\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, durationpy, banal, SQLAlchemy, pyproject_hooks, packaging, opentelemetry-util-http, mmh3, humanfriendly, chroma-hnswlib, bcrypt, backoff, asgiref, requests-toolbelt, posthog, coloredlogs, build, pydantic-settings, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, dataset, opentelemetry-instrumentation-asgi, langchain-core, sentence_transformers, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-huggingface, langchain, chromadb, langchain-community, langchain-chroma\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 2.0.30\n    Uninstalling SQLAlchemy-2.0.30:\n      Successfully uninstalled SQLAlchemy-2.0.30\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-1.4.54 asgiref-3.8.1 backoff-2.2.1 banal-1.0.6 bcrypt-4.2.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.15 coloredlogs-15.0.1 dataset-1.6.2 durationpy-0.9 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.4 langchain-chroma-0.1.4 langchain-community-0.3.3 langchain-core-0.3.13 langchain-huggingface-0.1.0 langchain-text-splitters-0.3.0 langsmith-0.1.137 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-util-http-0.46b0 packaging-24.1 posthog-3.7.0 pydantic-settings-2.6.0 pypika-0.48.9 pyproject_hooks-1.2.0 requests-toolbelt-1.0.0 sentence_transformers-3.2.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nCollecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank_bm25\nSuccessfully installed rank_bm25-0.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:01.341467Z","iopub.execute_input":"2024-10-28T17:53:01.342136Z","iopub.status.idle":"2024-10-28T17:53:01.882942Z","shell.execute_reply.started":"2024-10-28T17:53:01.342089Z","shell.execute_reply":"2024-10-28T17:53:01.882000Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\n# Set environment variables for LangSmith tracking\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  # Replace with your actual API key\nos.environ[\"LANGCHAIN_PROJECT\"] = \"moviechat\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:01.884328Z","iopub.execute_input":"2024-10-28T17:53:01.884965Z","iopub.status.idle":"2024-10-28T17:53:01.889938Z","shell.execute_reply.started":"2024-10-28T17:53:01.884919Z","shell.execute_reply":"2024-10-28T17:53:01.888869Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from IPython.display import display\nimport logging\nclass DisplayHandler(logging.Handler):\n    def emit(self, record):\n        display(self.format(record))\n\n# Configure logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Add custom display handler\ndisplay_handler = DisplayHandler()\ndisplay_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s:%(message)s'))\nlogger.addHandler(display_handler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:01.892024Z","iopub.execute_input":"2024-10-28T17:53:01.892299Z","iopub.status.idle":"2024-10-28T17:53:01.900995Z","shell.execute_reply.started":"2024-10-28T17:53:01.892269Z","shell.execute_reply":"2024-10-28T17:53:01.900183Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from langchain.document_loaders import CSVLoader\nfrom datasets import load_dataset, concatenate_datasets, Value\nfrom huggingface_hub import hf_hub_download\nimport ast\n\nfile_path1 = hf_hub_download(repo_id=\"niloufarna/MovieChat\", subfolder='imdb', filename=\"imdb_cornell_movie_details_dataset.csv\", local_dir= '.', repo_type=\"dataset\")\nfile_path2 = hf_hub_download(repo_id=\"niloufarna/MovieChat\", subfolder='imdb', filename=\"imdb_movieqa_details_dataset.csv\", local_dir ='.', repo_type=\"dataset\")\n\n\ndataset1 = load_dataset('csv', data_files = file_path1, split = 'train' )\ndataset2 = load_dataset('csv', data_files = file_path2, split = 'train' )\n\n# Remove rows where 'synopsis', 'plot', and 'plot_outline' are all NaN (None in Hugging Face)\nfiltered_dataset1 = dataset1.filter(\n    lambda row: (\n        row['plot_outline'] is not None and row['movie_name'] != 'Cherry Falls'\n))\n\n# Fill NaN values in 'synopsis' with the value from 'plot_outline'\ndef fill_movie_details(example):\n    if example['synopsis'] is None and example['plot_outline'] is not None:\n        example['synopsis'] = example['plot_outline']\n    if example['movie_name'] == 'spare me':\n        example['votes'] = 31\n        example['rating'] = 6.8\n    return example\n\ndef updata_dataset2(example):\n   if example['movie_name'] == 'Men in Black 3: Gag Reel':\n       example['plot_outline'] = \"The Men in Black 3: Gag Reel is a behind-the-scenes compilation of bloopers and funny moments during the filming of Men in Black 3. It features actors like Will Smith and Tommy Lee Jones breaking character, laughing through mistakes, and showcasing some lighthearted moments that occur during production. These reels are typically included as part of bonus material in the movie's DVD/Blu-ray release, offering fans a glimpse of the fun and camaraderie on set.\"\n   \n   return example\n\n# Apply the transformation function to the dataset\ndataset1 = filtered_dataset1.map(fill_movie_details)\n# Modify the 'votes' feature type to int64\ndataset1 = dataset1.cast_column('votes', Value('int64'))\n\n\n# Define a function to count null values (None) in each row\ndef count_nulls(example):\n    return {'null_count': sum(1 for value in example.values() if value is None)}\n\n# Apply the function to calculate null counts for each row\nnull_count_dataset = dataset1.map(count_nulls)\n\n# # Sum all null counts to get the total number of null values in the dataset\ntotal_nulls = sum(null_count_dataset['null_count'])\n\n# Print the total number of null values\nprint(f\"Total number of null values in the dataset: {total_nulls}\")\n\ndataset2 = dataset2.map(updata_dataset2)\n\ncombined_dataset = concatenate_datasets([dataset1, dataset2])\ndef convert_list_string(example):\n    # Parse the string representation of a list into an actual list\n   genres_list = ast.literal_eval(example['genres'])  # replace 'your_column_name' with the actual column name\n   languages_list = ast.literal_eval(example['languages'])\n   director_list = ast.literal_eval(example['director'])\n   writer_list = ast.literal_eval(example['writer'])\n   cast_list = ast.literal_eval(example['cast'])\n   character_names_list = ast.literal_eval(example['character_names'])\n   # Join the list elements into a single string\n   example['genres'] = ', '.join(genres_list)\n   example['languages'] = ', '.join(languages_list)\n   example['director'] = ', '.join(genres_list)\n   example['writer'] = ', '.join(languages_list)\n   example['cast'] = ', '.join(genres_list)\n   example['character_names'] = ', '.join(languages_list)\n\ncombined_dataset.map(convert_list_string)\n\npath = 'imdb_movie_dataset.csv'\ncombined_dataset.to_csv(path)\n\n\n# # Load CSV data from URLs\n# loader = CSVLoader(path)\n\n# documents = loader.load()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:01.902376Z","iopub.execute_input":"2024-10-28T17:53:01.902738Z","iopub.status.idle":"2024-10-28T17:53:08.180165Z","shell.execute_reply.started":"2024-10-28T17:53:01.902697Z","shell.execute_reply":"2024-10-28T17:53:08.179236Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:03,048:INFO:NumExpr defaulting to 4 threads.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:03,269:INFO:PyTorch version 2.4.0 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:03,274:INFO:Polars version 1.9.0 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:03,277:INFO:TensorFlow version 2.16.1 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:03,282:INFO:JAX version 0.4.26 available.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)b/imdb_cornell_movie_details_dataset.csv:   0%|          | 0.00/6.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67f27c893c74f3c81d04ee1443a65ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"imdb/imdb_movieqa_details_dataset.csv:   0%|          | 0.00/5.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04951dc3c8814db18a562b80508b6a53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6645cb5a03245d08a462291f2855872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3622ea458913488db6916ab61fbec77d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/615 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77d8a9c3cad4156a6288f1da51a42dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e261ab8c70040ac893d8120618a90ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3fa9041263e4482812055c9306c4ff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f3d1c41b424395b6b43da5c68aa22b"}},"metadata":{}},{"name":"stdout","text":"Total number of null values in the dataset: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f27f80057d48f38cd60fadc401f80e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae13e7c2f52e4cd39baa20d890de75d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9ed3c7723104a1dba2d2fc4c43252a9"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"12462581"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# !pip install redis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:08.181341Z","iopub.execute_input":"2024-10-28T17:53:08.181688Z","iopub.status.idle":"2024-10-28T17:53:08.185907Z","shell.execute_reply.started":"2024-10-28T17:53:08.181655Z","shell.execute_reply":"2024-10-28T17:53:08.185013Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import redis\n# import json\n\n# # Create a Redis connection\n\n\n# r = redis.Redis(\n#   host='redis-11169.c16.us-east-1-3.ec2.redns.redis-cloud.com',\n#   port=11169,\n#   password='vrntyCBNWv3k1133D7P3a79oIdYvLsIa')\n\n# # Define a function to store metadata in Redis\n# def store_metadata_in_redis(dataset):\n#     for row in dataset:\n#         # Create a unique key using movie_name and year\n#         key = f\"{row['movie_name']}:{row['year']}\"\n        \n#         # Store the metadata (excluding the synopsis/plot for now)\n#         metadata = {\n#             'rating': row.get('rating'),\n#             'votes': row.get('votes'),\n#             'genre': row.get('genre'),\n#             'director': row.get('director'),\n#             'writer': row.get('writer'),\n#             'cast': row.get('cast'),\n#             'character_names':row.get('character_names'),\n#             'languages': row.get('languages'),\n#             'countries':row.get('countries')\n#         }\n\n#         # Store the data in Redis as a JSON object\n#         r.set(key, json.dumps(metadata))\n\n# # Apply the function to the documents (this assumes `documents` is a list of dictionaries)\n# store_metadata_in_redis(combined_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:08.187256Z","iopub.execute_input":"2024-10-28T17:53:08.187672Z","iopub.status.idle":"2024-10-28T17:53:08.208516Z","shell.execute_reply.started":"2024-10-28T17:53:08.187628Z","shell.execute_reply":"2024-10-28T17:53:08.207564Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# # Test connection with Redis\n# try:\n#     r.ping()\n#     print(\"Connected to Redis!\")\n# except redis.ConnectionError as e:\n#     print(f\"Failed to connect to Redis: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:08.209729Z","iopub.execute_input":"2024-10-28T17:53:08.210044Z","iopub.status.idle":"2024-10-28T17:53:08.221954Z","shell.execute_reply.started":"2024-10-28T17:53:08.209991Z","shell.execute_reply":"2024-10-28T17:53:08.221107Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# def retrieve_metadata_from_redis(movie_name, year):\n#     key = f\"{movie_name}:{year}\"\n    \n#     # Retrieve the data from Redis\n#     metadata = r.get(key)\n    \n#     # If the key exists, return the metadata as a Python dictionary\n#     if metadata:\n#         return json.loads(metadata)\n#     else:\n#         print(f\"No data found for key: {key}\")\n#         return None\n\n# # Example of retrieving metadata for a movie\n# movie_name = 'a nightmare on elm street 4: the dream master'\n# year = 1988\n# metadata = retrieve_metadata_from_redis(movie_name, year)\n\n# if metadata:\n#     print(f\"Metadata for {movie_name} ({year}):\", metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:08.222992Z","iopub.execute_input":"2024-10-28T17:53:08.223284Z","iopub.status.idle":"2024-10-28T17:53:08.231897Z","shell.execute_reply.started":"2024-10-28T17:53:08.223253Z","shell.execute_reply":"2024-10-28T17:53:08.230943Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n# from langchain_chroma import Chroma\n# from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n# # Initialize embeddings and Chroma\n# device = 'cuda' if SentenceTransformer('all-MiniLM-L6-v2').device.type == 'cuda' else 'cpu'\n# model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n# chroma_index = Chroma(embedding_function=embedding_function,collection_name=\"movie_collection\", persist_directory=\"./chroma_data\")\n\n# # Define a text splitter\n# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n\n# # Concatenate plot, plot_outline, and synopsis into a single string\n# # documents = [\n# #     f\"Movie: {row['movie_name']} ({row['year']})\\nPlot: {row['plot']}\\nPlot Outline: {row['plot_outline']}\\nSynopsis: {row['synopsis']}\"\n# #     for row in combined_dataset\n# # ]\n\n\n\n# print(\"before chromadb\")\n# # Split the concatenated documents into chunks and embed them\n# # Define a batch size for processing\n# BATCH_SIZE = 50\n\n# # Placeholder for accumulating chunks and metadata in batches\n# batch_chunks = []\n# batch_metadata = []\n\n# for i,row in enumerate(combined_dataset):\n#     # Log each movie processed\n#     logger.debug(f\"Processing movie: {row['movie_name']}\")\n\n#     full_text = (\n#         f\"Movie Name: {row['movie_name']}\\n\"\n#         f\"Year: {row['year']}\\n\"\n#         f\"Rating: {row.get('rating')}\\n\"\n#         f\"Votes: {row.get('votes')}\\n\"\n#         f\"Genre: {row.get('genre')}\\n\"\n#         f\"Director: {row.get('director')}\\n\"\n#         f\"Writer: {row.get('writer')}\\n\"\n#         f\"Cast: {row.get('cast')}\\n\"\n#         f\"Character Names: {row.get('character_names')}\\n\"\n#         f\"Languages: {row.get('languages')}\\n\"\n#         f\"Countries: {row.get('countries')}\\n\"\n#         f\"Plot: {row['plot']}\\n\"\n#         f\"Plot Outline: {row['plot_outline']}\\n\"\n#         f\"Synopsis: {row['synopsis']}\"\n#     )\n\n#     metadata = {'movie_name': row['movie_name'], 'year': row['year']}\n#     chunks = text_splitter.split_text(full_text)\n    \n#     batch_chunks.extend(chunks)\n#     batch_metadata.extend([metadata] * len(chunks))\n\n#     # When batch size is reached, add to Chroma\n#     if len(batch_chunks) >= BATCH_SIZE:\n#         logger.info(f\"Adding batch of {len(batch_chunks)} chunks to Chroma.\")\n#         chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n#         chroma_index.persist()\n#         batch_chunks, batch_metadata = [], []  # Clear the batch lists\n\n# # Add remaining chunks in the last batch\n# if batch_chunks:\n#     logger.info(f\"Adding final batch of {len(batch_chunks)} chunks to Chroma.\")\n#     chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n#     chroma_index.persist()\n\n\n# logger.info(\"Embedding and adding texts to Chroma completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:08.235661Z","iopub.execute_input":"2024-10-28T17:53:08.235981Z","iopub.status.idle":"2024-10-28T17:53:08.242632Z","shell.execute_reply.started":"2024-10-28T17:53:08.235949Z","shell.execute_reply":"2024-10-28T17:53:08.241685Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nfrom langchain_chroma import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport multiprocessing as mp\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nimport torch\n\nBATCH_SIZE = 50\n\n# Initialize the embedding model with GPU support if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nembedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": device})\n\n\n# Initialize Chroma vector store\nchroma_index = Chroma(\n    embedding_function=embedding_function,\n    collection_name=\"movie_collection\",\n    persist_directory=\"./chroma_data\"\n)\n\n# Define a text splitter with optimized chunk size and overlap\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n\n# Function to process each movie\ndef process_movie(row):\n    full_text = (\n        f\"Movie Name: {row['movie_name']}\\n\"\n        f\"Year: {row['year']}\\n\"\n        f\"Rating: {row.get('rating')}\\n\"\n        f\"Votes: {row.get('votes')}\\n\"\n        f\"Genre: {row.get('genre')}\\n\"\n        f\"Director: {row.get('director')}\\n\"\n        f\"Writer: {row.get('writer')}\\n\"\n        f\"Cast: {row.get('cast')}\\n\"\n        f\"Character Names: {row.get('character_names')}\\n\"\n        f\"Languages: {row.get('languages')}\\n\"\n        f\"Countries: {row.get('countries')}\\n\"\n        f\"Plot: {row['plot']}\\n\"\n        f\"Plot Outline: {row['plot_outline']}\\n\"\n        f\"Synopsis: {row['synopsis']}\"\n    )\n    metadata = {\n        'movie_name': row['movie_name'],\n        'year': row['year']\n    }\n    chunks = text_splitter.split_text(full_text)\n    return chunks, [metadata] * len(chunks)\n\nlogger.info(\"Starting ChromaDB integration...\")\n\n# Initialize multiprocessing pool\npool = mp.Pool(mp.cpu_count())\nlogger.info(f\"Number of CPUs: {mp.cpu_count()}\")\n# Process movies in parallel\nresults = pool.map(process_movie, combined_dataset)\npool.close()\npool.join()\nlogger.info(f\"Size of results {len(results)}\")\n\n# Initialize batch lists\nbatch_chunks = []\nbatch_metadata = []\n\n# Iterate through processed results\nfor chunks, metadatas in results:\n    batch_chunks.extend(chunks)\n    batch_metadata.extend(metadatas)\n    \n    if len(batch_chunks) >= BATCH_SIZE:\n        logger.info(f\"Adding batch of {len(batch_chunks)} chunks to Chroma.\")\n        chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n        batch_chunks, batch_metadata = [], []  # Clear the batch lists\n\n# Add remaining chunks\nif batch_chunks:\n    logger.info(f\"Adding final batch of {len(batch_chunks)} chunks to Chroma.\")\n    chroma_index.add_texts(batch_chunks, metadatas=batch_metadata)\n    \n\nlogger.info(\"Embedding and adding texts to Chroma completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:53:08.243758Z","iopub.execute_input":"2024-10-28T17:53:08.244031Z","iopub.status.idle":"2024-10-28T17:55:01.618000Z","shell.execute_reply.started":"2024-10-28T17:53:08.244002Z","shell.execute_reply":"2024-10-28T17:55:01.617007Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:27,013:INFO:Load pretrained SentenceTransformer: all-MiniLM-L6-v2'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80daf97e25c44a5d867e960c7cf0bef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f13b654c320d4277895f1b68e0dbaedc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb4f7ced401e4784b967f8ceb734c882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556122027a0f485cb47e07179b6bddfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d12350b7b434bd8b4a680d34cd7a41c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e01dec7a9d748c08a7d4470f20aae96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4323f65772274d5ea487efdc42f6f062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4145c43fb85346949f00eaad1cdaaf42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40bfabe72935497fa60acbb40b199834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67d58e68c7c419db8dca1548b94feeb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec8890072224abba81359a925c196e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:31,550:INFO:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:32,156:INFO:Starting ChromaDB integration...'"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:32,224:INFO:Number of CPUs: 4'"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:34,255:INFO:Size of results 1002'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:34,258:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:34,970:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:35,189:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:35,399:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:35,607:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:35,800:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:36,050:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:36,256:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:36,434:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:36,625:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:36,823:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:37,081:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:37,308:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:37,533:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:37,758:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:38,037:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:38,223:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:38,420:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:38,706:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:38,939:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:39,191:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:39,487:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:39,679:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:39,876:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:40,076:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:40,318:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:40,576:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:40,803:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:41,014:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:41,266:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:41,500:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:41,713:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:41,935:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:42,166:INFO:Adding batch of 84 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:42,548:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:42,799:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:43,021:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:43,272:INFO:Adding batch of 75 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:43,633:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:43,869:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:44,092:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:44,334:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:44,615:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:44,890:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:45,150:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:45,347:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:45,559:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:45,786:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:46,005:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:46,223:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:46,533:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:46,764:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:47,029:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:47,229:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:47,426:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:47,647:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:47,846:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:48,134:INFO:Adding batch of 89 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:48,639:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:48,891:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:49,152:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:49,412:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:49,736:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:50,145:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:50,398:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:50,662:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:51,004:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:51,228:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:51,439:INFO:Adding batch of 77 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:51,714:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:51,950:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:52,142:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:52,351:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:52,571:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:52,790:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:53,050:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:53,346:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:53,660:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:53,976:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:54,266:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:54,512:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:54,758:INFO:Adding batch of 84 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:55,056:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:55,355:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:55,613:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:55,832:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:56,052:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:56,289:INFO:Adding batch of 75 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:56,574:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:56,770:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:56,955:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:57,173:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:57,382:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:57,677:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:57,978:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:58,171:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:58,378:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:58,615:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:58,893:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:59,197:INFO:Adding batch of 89 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:59,503:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:59,699:INFO:Adding batch of 79 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:53:59,995:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:00,204:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:00,474:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:00,672:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:00,919:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:01,182:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:01,407:INFO:Adding batch of 75 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:01,683:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:01,917:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:02,214:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:02,455:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:02,667:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:02,921:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:03,215:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:03,888:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:04,072:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:04,285:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:04,485:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:04,724:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:04,975:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:05,239:INFO:Adding batch of 84 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:05,560:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:05,780:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:05,985:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:06,202:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:06,380:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:06,581:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:06,871:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:07,108:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:07,440:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:07,649:INFO:Adding batch of 94 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:08,028:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:08,240:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:08,448:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:08,699:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:08,939:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:09,190:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:09,454:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:09,655:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:09,867:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:10,098:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:10,320:INFO:Adding batch of 76 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:10,594:INFO:Adding batch of 90 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:10,949:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:11,189:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:11,625:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:11,964:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:12,240:INFO:Adding batch of 82 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:12,560:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:12,768:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:12,951:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:13,193:INFO:Adding batch of 89 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:13,533:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:13,727:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:13,923:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:14,110:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:14,358:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:14,539:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:14,773:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:15,010:INFO:Adding batch of 78 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:15,282:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:15,593:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:15,875:INFO:Adding batch of 72 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:16,140:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:16,382:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:16,610:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:16,850:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:17,074:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:17,296:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:17,558:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:17,748:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:17,982:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:18,235:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:18,468:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:18,713:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:18,976:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:19,192:INFO:Adding batch of 87 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:19,610:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:19,874:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:20,163:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:20,366:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:20,582:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:20,808:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:20,998:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:21,261:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:21,480:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:21,721:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:21,931:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:22,152:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:22,386:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:22,650:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:22,848:INFO:Adding batch of 92 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:23,188:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:23,509:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:23,727:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:23,963:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:24,177:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:24,398:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:24,692:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:24,981:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:25,239:INFO:Adding batch of 79 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:25,522:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:25,757:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:25,999:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:26,259:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:26,491:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:26,715:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:26,953:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:27,199:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:27,505:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:27,704:INFO:Adding batch of 91 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:28,030:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:28,272:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:28,482:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:28,722:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:29,058:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:29,363:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:29,634:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:29,925:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:30,247:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:30,502:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:30,720:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:30,971:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:31,182:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:31,425:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:31,787:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:32,002:INFO:Adding batch of 71 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:32,263:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:32,516:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:32,728:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:32,969:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:33,213:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:33,439:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:33,700:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:33,987:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:34,184:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:34,410:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:34,637:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:34,900:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:35,105:INFO:Adding batch of 71 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:35,372:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:35,723:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:35,921:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:36,173:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:36,425:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:36,637:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:36,862:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:37,104:INFO:Adding batch of 76 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:37,374:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:37,647:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:37,875:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:38,177:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:38,393:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:38,654:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:38,881:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:39,089:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:39,321:INFO:Adding batch of 57 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:39,532:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:39,905:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:40,177:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:40,364:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:40,628:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:40,867:INFO:Adding batch of 71 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:41,144:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:41,392:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:41,646:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:41,877:INFO:Adding batch of 124 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:42,320:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:42,558:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:42,828:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:43,028:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:43,276:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:43,481:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:43,862:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:44,062:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:44,317:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:44,517:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:44,747:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:45,025:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:45,270:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:45,508:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:45,732:INFO:Adding batch of 61 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:45,987:INFO:Adding batch of 69 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:46,248:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:46,490:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:46,715:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:47,029:INFO:Adding batch of 65 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:47,313:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:47,646:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:48,115:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:48,426:INFO:Adding batch of 64 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:48,768:INFO:Adding batch of 81 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:49,169:INFO:Adding batch of 74 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:49,537:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:49,897:INFO:Adding batch of 103 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:50,417:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:50,677:INFO:Adding batch of 107 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:51,207:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:51,498:INFO:Adding batch of 80 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:51,901:INFO:Adding batch of 77 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:52,361:INFO:Adding batch of 122 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:52,868:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:53,296:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:53,506:INFO:Adding batch of 63 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:53,761:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:53,998:INFO:Adding batch of 60 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:54,232:INFO:Adding batch of 67 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:54,491:INFO:Adding batch of 68 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:54,735:INFO:Adding batch of 54 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:54,941:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:55,166:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:55,371:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:55,606:INFO:Adding batch of 77 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:55,875:INFO:Adding batch of 112 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:56,311:INFO:Adding batch of 62 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:56,539:INFO:Adding batch of 51 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:56,734:INFO:Adding batch of 73 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:57,033:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:57,442:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:57,642:INFO:Adding batch of 59 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:57,856:INFO:Adding batch of 53 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:58,074:INFO:Adding batch of 76 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:58,376:INFO:Adding batch of 55 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:58,574:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:58,780:INFO:Adding batch of 80 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:59,065:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:59,353:INFO:Adding batch of 70 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:59,621:INFO:Adding batch of 52 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:54:59,816:INFO:Adding batch of 66 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:00,074:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:00,327:INFO:Adding batch of 56 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:00,538:INFO:Adding batch of 58 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:00,770:INFO:Adding batch of 50 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:00,958:INFO:Adding batch of 95 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:01,471:INFO:Adding final batch of 27 chunks to Chroma.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:55:01,613:INFO:Embedding and adding texts to Chroma completed.'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from langchain.retrievers import EnsembleRetriever, BM25Retriever\nretriever_vanilla = chroma_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\nretriever_mmr = chroma_index.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\nretriever_BM25 = BM25Retriever.from_texts(chunks, search_kwargs={\"k\": 3})\n# initialize the ensemble retriever with 3 Retrievers\nensemble_retriever = EnsembleRetriever(\n    retrievers=[retriever_vanilla, retriever_mmr, retriever_BM25], weights=[0.3, 0.3, 0.4]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:55:01.619413Z","iopub.execute_input":"2024-10-28T17:55:01.620128Z","iopub.status.idle":"2024-10-28T17:55:01.802649Z","shell.execute_reply.started":"2024-10-28T17:55:01.620093Z","shell.execute_reply":"2024-10-28T17:55:01.801862Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\nfrom langchain.memory import ConversationSummaryMemory\n\n# Hugging Face Hub integration\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,pipeline,BitsAndBytesConfig\n\n# Define the model ID\nmodel_id = \"NousResearch/Llama-2-7b-chat-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")#, quantization_config=quantization_config)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, top_k=30, temperature=0.5)\nllm = HuggingFacePipeline(pipeline=pipe, verbose=True, pipeline_kwargs=dict(\n        # max_new_tokens=512,\n        # repetition_penalty=1.03,\n        return_full_text=False,\n    ),)\n\n# Initialize the Hugging Face Hub LLM for the BART model\nsummary_llm = HuggingFaceEndpoint(\n    repo_id=\"facebook/bart-large-cnn\",  # BART model for summarization\n   temperature= 0.7  # Adjust parameters if needed\n)\n\n# Initialize the conversation memory using the summarization model\nmemory = ConversationSummaryMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"response\", llm=summary_llm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:55:01.803707Z","iopub.execute_input":"2024-10-28T17:55:01.803995Z","iopub.status.idle":"2024-10-28T17:56:32.050094Z","shell.execute_reply.started":"2024-10-28T17:55:01.803964Z","shell.execute_reply":"2024-10-28T17:56:32.049172Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73334bc9830241d6b7a175d4b207405e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5538f44e9d047d09ea67b82bb83ba54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff56edeefdc4bb0998072a77f9e08a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac451df34ca4abd8c22fe41fe4e96a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be3d37f89e8449ab892df0a4dfca7281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e790bc973bb0434d8d331d9c9e388e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d2575f3c5ad48a587c05a38690efe2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0aecb5d786c4b908b024a0378c3f0cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28710ff63884ae0b0e2ed5fa03aa2d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7eb8aedac240dab2a25a06c54c57df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-28 17:56:13,483:INFO:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee2dd13a1a94414b3ea05b549fd0355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8b60224c9246e28d4811821579adfc"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/3514978564.py:29: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationSummaryMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"response\", llm=summary_llm)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"memory.load_memory_variables({})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.051275Z","iopub.execute_input":"2024-10-28T17:56:32.051589Z","iopub.status.idle":"2024-10-28T17:56:32.057790Z","shell.execute_reply.started":"2024-10-28T17:56:32.051549Z","shell.execute_reply":"2024-10-28T17:56:32.056932Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'chat_history': [SystemMessage(content='', additional_kwargs={}, response_metadata={})]}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# from langchain.prompts import ChatPromptTemplate\n\n# # Define a prompt template for conversational tasks\n# template = \"\"\"\n# You are a helpful assistant. Please answer the following question:\n\n# Question: {question}\n# \"\"\"\n# #  \"\"\"Instruction:{}\n# # Input:{}\n# # Context:{}\n# # \"\"\"\n# prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.058950Z","iopub.execute_input":"2024-10-28T17:56:32.059290Z","iopub.status.idle":"2024-10-28T17:56:32.906920Z","shell.execute_reply.started":"2024-10-28T17:56:32.059250Z","shell.execute_reply":"2024-10-28T17:56:32.905945Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Define the human message prompt template\n# human_message_template = PromptTemplate.from_template(\n#     \"\"\"\n#     You are an experienced assistant specializing in question-answering tasks. \n#     Utilize the provided context to respond to the question. \n#     If the answer is unknown, always state 'I don't know.' \n#     Never provide an answer you are unsure about and ensure it is concise.\n#     Your answer must be comprehensive and contain all of the relevant details in the Context.\n#     \\nQuestion: {question} \\nContext: {context} \\nResponse:\n#     \"\"\"\n# )\n\n# # Create a HumanMessagePromptTemplate instance using the defined prompt template\n# human_message_prompt_template = HumanMessagePromptTemplate(prompt=human_message_template)\n\n# # Create the ChatPromptTemplate with the input variables and messages, excluding metadata\n# chat_prompt_template = ChatPromptTemplate(\n#     input_variables=['context', 'input'],\n#     messages=[human_message_prompt_template]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.908274Z","iopub.execute_input":"2024-10-28T17:56:32.908660Z","iopub.status.idle":"2024-10-28T17:56:32.918543Z","shell.execute_reply.started":"2024-10-28T17:56:32.908621Z","shell.execute_reply":"2024-10-28T17:56:32.917778Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# from langchain.prompts import PromptTemplate\n# from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n# # Define the prompt template with concise instructions\n# # Define the system message prompt template\n# system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n#     \"\"\"You are an experienced assistant specializing in question-answering tasks.\n# Use the context to answer the following question.\n# If you're unsure, say 'I don't know.'\n# \\nInstruction: {instruction}\\n\n#     \"\"\")\n# # Define the human message prompt for the user's question\n# human_message_prompt_template = HumanMessagePromptTemplate.from_template(\"Question: {question}\")\n\n# prompt_template = PromptTemplate.from_template(\n# \"\"\"Chat history: {chat_history}\\nContext: {context}\n#     \"\"\"\n# )\n\n\n# chat_prompt_template = ChatPromptTemplate(\n#     input_variables=['instruction', 'question'],\n#     messages=[system_message_prompt_template,\n#             message_prompt_template,\n#             # human_message_prompt_template\n#              ]\n# )\n\n# chat_prompt_template2 = (\n#     +prompt_template\n#     + \"\\nResponse:\"\n# )\n\n# prompt = PipelinePromptTemplate(\n#     final_prompt=PromptTemplate.from_template(\"{context_prompt_template}\"),\n#     pipeline_prompts=[\n#         ('context_prompt_template', context_prompt_template)  # This will generate 'context_text' variable\n#         ('chat_prompt_template':)\n#     ]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.919588Z","iopub.execute_input":"2024-10-28T17:56:32.919907Z","iopub.status.idle":"2024-10-28T17:56:32.932831Z","shell.execute_reply.started":"2024-10-28T17:56:32.919876Z","shell.execute_reply":"2024-10-28T17:56:32.932048Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# chat_prompt_template = PromptTemplate.from_template(\n#     \"\"\"You are an experienced assistant specializing in question-answering tasks.\n# Use the context to answer the following question.\n\n# \\nInstruction: {instruction}\\nChat history: {chat_history}\\nContext: {context}Question: {question}\\nResponse:\"\n#     \"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T18:02:34.391260Z","iopub.execute_input":"2024-10-28T18:02:34.391975Z","iopub.status.idle":"2024-10-28T18:02:34.396292Z","shell.execute_reply.started":"2024-10-28T18:02:34.391934Z","shell.execute_reply":"2024-10-28T18:02:34.395335Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# prompt = (\n#     PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n#     + \", make it funny\"\n#     + \"\\n\\nand in {language}\"\n# )\n\n\n# prompt.format(topic=\"sports\", language=\"Afrikaans\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.948447Z","iopub.execute_input":"2024-10-28T17:56:32.949056Z","iopub.status.idle":"2024-10-28T17:56:32.957419Z","shell.execute_reply.started":"2024-10-28T17:56:32.949013Z","shell.execute_reply":"2024-10-28T17:56:32.956538Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# formatted_prompt = chat_prompt_template.format(\n#     instruction=\"Please provide the answer based on the given context.\",\n#     question=\"What is the capital of France?\",\n#     chat_history=\"User: What is the largest continent? Assistant: Asia\",\n#     context=\"France is a country in Europe. The capital of France is Paris.\"\n# )\n\n# print(formatted_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.958551Z","iopub.execute_input":"2024-10-28T17:56:32.959168Z","iopub.status.idle":"2024-10-28T17:56:32.968155Z","shell.execute_reply.started":"2024-10-28T17:56:32.959135Z","shell.execute_reply":"2024-10-28T17:56:32.967376Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n# Define the prompt template with concise instructions\n# Define the system message prompt template\nsystem_message_prompt_template = SystemMessagePromptTemplate.from_template(\n    \"\"\"You are an experienced assistant specializing in question-answering tasks.\nUse the context to answer the following question.\nIf you're unsure, say 'I don't know.'\n\\nInstruction: {instruction}\\n\n    \"\"\")\n# Define the human message prompt for the user's question\nhuman_message_prompt_template = HumanMessagePromptTemplate.from_template(\"Question: {question}\")\n\ncontext_message_prompt_template = AIMessagePromptTemplate.from_template(\n\"\"\"Chat history: {chat_history}\\nContext: {context}\n    \"\"\"\n)\n\n\nchat_prompt_template = (ChatPromptTemplate(\n    input_variables=['instruction', 'question','chat_history','context'],\n    messages=[system_message_prompt_template,\n            context_message_prompt_template,\n            human_message_prompt_template\n             ]\n)\n+\"\\nResponse: \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.969149Z","iopub.execute_input":"2024-10-28T17:56:32.969752Z","iopub.status.idle":"2024-10-28T17:56:32.983021Z","shell.execute_reply.started":"2024-10-28T17:56:32.969710Z","shell.execute_reply":"2024-10-28T17:56:32.982283Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# # Example question to pass to the chain\n# question_data = {\n#     \"instruction\": \"Answer the following question:\",\n#     \"question\": \"How does Frodo come to own Bilbo's Ring?\",\n#     \"context\": \"\",  # Placeholder context if no documents are retrieved\n#     \"chat_history\": \"\"  # Placeholder chat history if no history is available\n# }\n\nchat_prompt_template.pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:56:32.984007Z","iopub.execute_input":"2024-10-28T17:56:32.984257Z","iopub.status.idle":"2024-10-28T17:56:32.997130Z","shell.execute_reply.started":"2024-10-28T17:56:32.984229Z","shell.execute_reply":"2024-10-28T17:56:32.996326Z"}},"outputs":[{"name":"stdout","text":"================================\u001b[1m System Message \u001b[0m================================\n\nYou are an experienced assistant specializing in question-answering tasks.\nUse the context to answer the following question.\nIf you're unsure, say 'I don't know.'\n\nInstruction: \u001b[33;1m\u001b[1;3m{instruction}\u001b[0m\n\n    \n\n==================================\u001b[1m AI Message \u001b[0m==================================\n\nChat history: \u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\nContext: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n    \n\n================================\u001b[1m Human Message \u001b[0m=================================\n\nQuestion: \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n\n================================\u001b[1m Human Message \u001b[0m=================================\n\n\nResponse: \n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\n\n# First, load the memory to access chat history\nloaded_memory = RunnablePassthrough.assign(\n    chat_history=RunnableLambda(memory.load_memory_variables) | RunnableLambda(itemgetter(\"chat_history\"))\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T18:02:42.890628Z","iopub.execute_input":"2024-10-28T18:02:42.891025Z","iopub.status.idle":"2024-10-28T18:02:42.896388Z","shell.execute_reply.started":"2024-10-28T18:02:42.890987Z","shell.execute_reply":"2024-10-28T18:02:42.895439Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from langchain_core.output_parsers.string import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\nfrom langchain.schema import SystemMessage\nimport re\n# Function to format documents\n\ndef format_docs(docs):\n    # Ensure docs is a list of Document objects with a 'page_content' attribute\n    return \"\\n\".join([d.page_content for d in docs if hasattr(d, \"page_content\")])\ndef extract_system_message_content(input_dict):\n    if isinstance(input_dict, dict):\n        return [msg.content for msg in input_dict['chat_history'] if isinstance(msg, SystemMessage)]\n    return []\ndef extract_response(response: str) -> str:\n    # Assuming the answer starts after a specific keyword or pattern\n    match = re.search(r'Response:\\s*(.*)', response, re.DOTALL)\n    if match:\n        return match.group(1).strip()\n    return response.strip()\n\n# Chain definition\nconversation_chain = (\n   RunnableParallel( {\n        \"instruction\": RunnableLambda(lambda inputs: inputs[\"instruction\"]),\n        \"context\": RunnableLambda(lambda inputs: inputs[\"question\"])  # Extract \"question\" key for the retriever\n        | ensemble_retriever | format_docs,  # Retrieve and format documents as context\n        \"question\": RunnableLambda(lambda inputs: inputs[\"question\"]),   # Pass the user's question through\n        \"chat_history\": loaded_memory | extract_system_message_content\n    })\n    # The dictionary ends here, and chaining follows\n    | chat_prompt_template  # Use a prompt to handle the user question\n    | llm  # Model to generate the answer\n    # |extract_response\n    | StrOutputParser() # Parse the model's output as a string\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T18:02:44.590852Z","iopub.execute_input":"2024-10-28T18:02:44.591526Z","iopub.status.idle":"2024-10-28T18:02:44.601636Z","shell.execute_reply.started":"2024-10-28T18:02:44.591486Z","shell.execute_reply":"2024-10-28T18:02:44.600603Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# from langchain.callbacks.tracers import LangChainTracer\n# Example question to pass to the chain\nquestion_data = {\n    \"instruction\": \"Answer the following question:\",\n    \"question\": \"What is the name of the Dark Lord?\",#\"How does Frodo come to own Bilbo's Ring?\",\n    # \"context\": \"\",  # Placeholder context if no documents are retrieved\n    # \"chat_history\": \"\"  # Placeholder chat history if no history is available\n}\n\n\n# tracer = LangChainTracer(project_name=\"moviechat\")\n\n# Run the chain with the provided question data\nresponse = conversation_chain.invoke(question_data)#,config={\"callbacks\": [tracer]})\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T18:02:47.650962Z","iopub.execute_input":"2024-10-28T18:02:47.651878Z","iopub.status.idle":"2024-10-28T18:03:19.208835Z","shell.execute_reply.started":"2024-10-28T18:02:47.651835Z","shell.execute_reply":"2024-10-28T18:03:19.207709Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"You are an experienced assistant specializing in question-answering tasks.\nUse the context to answer the following question.\n\n\nInstruction: Answer the following question:\nChat history: ['']\nContext: Character Names: ['Aldor', 'Sam', 'Madril', 'Man Flesh Uruk', 'Galadriel', 'Legolas', 'Pippin', 'Eothain', 'Wormtongue', 'Haleth', 'Theoden', 'Gamling', 'Theodred', 'Saruman', 'Ugluk', 'Háma', 'Mauhur', 'Morwen', 'Gandalf', 'Merry', 'Aragorn', 'Eowyn', 'Haldir', 'Rohan Soldier', 'Mordor Orc', 'Gollum', 'Freda', 'Bereg', 'Arwen', 'Eomer', 'Grishnakh', 'Elrond', 'Faramir', 'Frodo', 'Hero Orc', 'Hero Orc', 'Hero Orc', 'Cute Rohan Refugee Child', 'Cute Rohan Refugee Child', 'Boromir', 'Wildman', 'Denethor', 'Ranger 1', 'Uruk-hai', 'Elven Warrior', 'Rivendell Elf', 'Rohan Soldier', 'Rohan Woman', 'Cute Rohan Refugee Child', 'Damrod', 'Rohan Refugee', 'Uruk-hai', 'Uruk-hai', 'Village Girl', 'Elf', 'Uruk-hai', 'Rohan Refugee', 'Uruk-hai', 'Uruk-hai', 'Easterling Warrior', 'Rohan Soldier', 'Rohan\nGraysmith tracks Allen down to a Vallejo hardware store, where he is employed as a sales clerk. After Allen asks if he can help Graysmith with anything, they stare at each other for a moment with blank expressions, during which time Allen\\'s name tag can be seen with \"Lee\" written on it, before Graysmith simply replies with a \"No\", and leaves the hardware store.Eight years later, in 1991, Mageau (Jimmi Simpson) meets with authorities and identifies Allen from a police mugshot. Shortly before, as the authorities walk by a bookshelf, copies of Robert Graysmith\\'s book Zodiac is spotted in the shelf for bestsellers.\\nFinal title cards, however, inform the audience that Allen died in 1992 before he could be questioned further by police, and DNA tests performed in 2002 did not match samples\nPlot Outline: A serial killer in the San Francisco Bay Area taunts police with his letters and cryptic messages. We follow the investigators and reporters in this lightly fictionalized account of the true 1970s' case as they search for the murderer, becoming obsessed with the case. Based on Robert Graysmith's book, the movie's focus is the lives and careers of the detectives and newspaper people.\nthe prime suspect is known to read, and he allows himself to be interviewed on television about his book-in-progress concerning the case. Obsessing over the unsolved case, he begins receiving anonymous phone calls with heavy breathing (on the night of Ferrin\\'s death, Graysmith discovered that someone prank-called the victim\\'s family and did the same thing). Because of his submersion in the case, Graysmith loses his job and his wife Melanie (Chloë Sevigny) leaves him, taking their children with her.Graysmith persistently contacts Toschi about the Zodiac murders and eventually impresses the veteran detective with his knowledge of the case. While Toschi cannot directly give Graysmith access to the information he discovered over the years, he provides contacts of other police departments in\nSynopsis: ['The film opens on July 4, 1969, with the Zodiac killer\\'s second attack, the shooting of Darlene Ferrin (Ciara Hughes) and Mike Mageau (Lee Norris) at a lovers\\' lane in Vallejo, California. Mageau survives while Ferrin dies from her injuries.One month later, a letter written by the Zodiac arrives at the San Francisco Chronicle. Paul Avery (Robert Downey, Jr.) is a Chronicle crime reporter. Robert Graysmith (Jake Gyllenhaal) is a political cartoonist there. The newspaper receives encrypted letters that the killer sends, taunting the police. Because of Graysmith\\'s status as a cartoonist, he is not taken seriously by Avery and the editors and is excluded from the initial details about the killings despite his interest in the case. In particular, he is drawn to the encrypted\nCharacter Names: ['Gandalf', 'Bilbo', 'Thorin', 'Balin', 'Dwalin', 'Bofur', 'Bombur', 'Fili', 'Kili', 'Oin', 'Nori', 'Ori', 'Old Bilbo', 'Frodo', 'Elrond', 'Galadriel', 'Saruman', 'Gollum', 'Radagast', 'Great Goblin', 'Thror', 'Thrain', 'Thranduil', 'Azog', 'Bolg', 'Yazneg', 'Master Worrywort', 'Lindir', 'Goblin Scribe', 'Necromancer', 'Dwarf Miner', 'Young Thrain', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Little Bilbo', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Cute Young Hobbit', 'Moria Orc',\nCharacter Names: ['Alex', 'Mr Alexander', 'Chief Guard', 'Dim', 'Stage Actor', 'Mrs. Alexander', 'Dr. Brodsky', 'Tramp', 'Joe the Lodger', 'Prison Governor', 'Catlady', 'Georgie', 'Deltoid', 'Prison Chaplain', 'Mum', 'Dr. Branom', 'Conspirator', 'Minister', 'Dad', 'Psychiatrist', 'Conspirator', 'Det. Const. Tom', 'Police Inspector', 'Pete', 'Julian', 'Dr. Alcott, Ludovico Center check-in', 'Handmaiden in Bible Fantasy', 'Sophisto', \"Junior Minister - Minister Frederick's Aid\", 'Detective sergeant', 'Handmaiden in Bible Fantasy', 'Billyboy', 'Handmaiden in Bible fantasy', 'Nurse Feeley', 'Desk Sergeant', \"Victim of Billyboy's Gang\", 'Sonietta', 'Doctor', 'Nurse', 'Stage Actress', 'Prison Check-in Officer', 'Girl in Ascot Fantasy', 'Mustachioed Ludovico Technician', 'Smiley Prisoner Next to\nCharacter Names: ['Lady Jessica', \"The Baron's Doctor\", 'Piter De Vries', 'Padishah Emperor Shaddam IV', 'Shadout Mapes', 'Thufir Hawat', 'Duncan Idaho', 'Paul Atreides', 'Princess Irulan', 'Reverend Mother Ramallo', 'Stilgar', 'Baron Vladimir Harkonnen', 'Nefud', 'Reverend Mother Gaius Helen Mohiam', 'Duke Leto Atreides', 'The Beast Rabban', 'Gurney Halleck', 'Feyd Rautha', 'Doctor Wellington Yueh', 'Doctor Kynes', 'Alia', 'Chani', 'Orlop', 'Otheym', 'Jamis', 'Harah', 'Bene Gesserit Sister', 'Little Fremen Boy', 'Fremen', 'Guard, House Atreides', 'Czigo', 'Bit Part', 'Imperial General', \"Harkonnen's Victim\", 'Spice Worker', 'Fremen Boy', 'Kinet', 'Palace Maid', 'Elderly man', 'Narrator: TV version', 'Fremen', 'Fremen Girl', 'Palace Maid', 'Fedaykin Fighter', \"Lady Jessica's Maid\", 'Bit\nChiseller', 'Prisoner', 'Prisoner', 'Prisoner', 'Prisoner', 'Hy-Brasilian', 'Prisoner', 'Odin', 'Thor', 'Horribly Slain Warrior', 'Even More Horribly Slain Warrior', \"Sven's Grandfather\", 'Child God']Question: What is the name of the Dark Lord?\nResponse:\"\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"SystemMessage(content=response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T17:57:07.056231Z","iopub.execute_input":"2024-10-28T17:57:07.057006Z","iopub.status.idle":"2024-10-28T17:57:07.063734Z","shell.execute_reply.started":"2024-10-28T17:57:07.056963Z","shell.execute_reply":"2024-10-28T17:57:07.062835Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"SystemMessage(content='System: You are an experienced assistant specializing in question-answering tasks.\\nUse the context to answer the following question.\\nIf you\\'re unsure, say \\'I don\\'t know.\\'\\n\\nInstruction: Answer the following question:\\n\\n    \\nAI: Chat history: [\\'\\']\\nContext: Character Names: [\\'Aldor\\', \\'Sam\\', \\'Madril\\', \\'Man Flesh Uruk\\', \\'Galadriel\\', \\'Legolas\\', \\'Pippin\\', \\'Eothain\\', \\'Wormtongue\\', \\'Haleth\\', \\'Theoden\\', \\'Gamling\\', \\'Theodred\\', \\'Saruman\\', \\'Ugluk\\', \\'Háma\\', \\'Mauhur\\', \\'Morwen\\', \\'Gandalf\\', \\'Merry\\', \\'Aragorn\\', \\'Eowyn\\', \\'Haldir\\', \\'Rohan Soldier\\', \\'Mordor Orc\\', \\'Gollum\\', \\'Freda\\', \\'Bereg\\', \\'Arwen\\', \\'Eomer\\', \\'Grishnakh\\', \\'Elrond\\', \\'Faramir\\', \\'Frodo\\', \\'Hero Orc\\', \\'Hero Orc\\', \\'Hero Orc\\', \\'Cute Rohan Refugee Child\\', \\'Cute Rohan Refugee Child\\', \\'Boromir\\', \\'Wildman\\', \\'Denethor\\', \\'Ranger 1\\', \\'Uruk-hai\\', \\'Elven Warrior\\', \\'Rivendell Elf\\', \\'Rohan Soldier\\', \\'Rohan Woman\\', \\'Cute Rohan Refugee Child\\', \\'Damrod\\', \\'Rohan Refugee\\', \\'Uruk-hai\\', \\'Uruk-hai\\', \\'Village Girl\\', \\'Elf\\', \\'Uruk-hai\\', \\'Rohan Refugee\\', \\'Uruk-hai\\', \\'Uruk-hai\\', \\'Easterling Warrior\\', \\'Rohan Soldier\\', \\'Rohan\\nGraysmith tracks Allen down to a Vallejo hardware store, where he is employed as a sales clerk. After Allen asks if he can help Graysmith with anything, they stare at each other for a moment with blank expressions, during which time Allen\\\\\\'s name tag can be seen with \"Lee\" written on it, before Graysmith simply replies with a \"No\", and leaves the hardware store.Eight years later, in 1991, Mageau (Jimmi Simpson) meets with authorities and identifies Allen from a police mugshot. Shortly before, as the authorities walk by a bookshelf, copies of Robert Graysmith\\\\\\'s book Zodiac is spotted in the shelf for bestsellers.\\\\nFinal title cards, however, inform the audience that Allen died in 1992 before he could be questioned further by police, and DNA tests performed in 2002 did not match samples\\nPlot Outline: A serial killer in the San Francisco Bay Area taunts police with his letters and cryptic messages. We follow the investigators and reporters in this lightly fictionalized account of the true 1970s\\' case as they search for the murderer, becoming obsessed with the case. Based on Robert Graysmith\\'s book, the movie\\'s focus is the lives and careers of the detectives and newspaper people.\\nthe prime suspect is known to read, and he allows himself to be interviewed on television about his book-in-progress concerning the case. Obsessing over the unsolved case, he begins receiving anonymous phone calls with heavy breathing (on the night of Ferrin\\\\\\'s death, Graysmith discovered that someone prank-called the victim\\\\\\'s family and did the same thing). Because of his submersion in the case, Graysmith loses his job and his wife Melanie (Chloë Sevigny) leaves him, taking their children with her.Graysmith persistently contacts Toschi about the Zodiac murders and eventually impresses the veteran detective with his knowledge of the case. While Toschi cannot directly give Graysmith access to the information he discovered over the years, he provides contacts of other police departments in\\nSynopsis: [\\'The film opens on July 4, 1969, with the Zodiac killer\\\\\\'s second attack, the shooting of Darlene Ferrin (Ciara Hughes) and Mike Mageau (Lee Norris) at a lovers\\\\\\' lane in Vallejo, California. Mageau survives while Ferrin dies from her injuries.One month later, a letter written by the Zodiac arrives at the San Francisco Chronicle. Paul Avery (Robert Downey, Jr.) is a Chronicle crime reporter. Robert Graysmith (Jake Gyllenhaal) is a political cartoonist there. The newspaper receives encrypted letters that the killer sends, taunting the police. Because of Graysmith\\\\\\'s status as a cartoonist, he is not taken seriously by Avery and the editors and is excluded from the initial details about the killings despite his interest in the case. In particular, he is drawn to the encrypted\\nCharacter Names: [\\'Gandalf\\', \\'Bilbo\\', \\'Thorin\\', \\'Balin\\', \\'Dwalin\\', \\'Bofur\\', \\'Bombur\\', \\'Fili\\', \\'Kili\\', \\'Oin\\', \\'Nori\\', \\'Ori\\', \\'Old Bilbo\\', \\'Frodo\\', \\'Elrond\\', \\'Galadriel\\', \\'Saruman\\', \\'Gollum\\', \\'Radagast\\', \\'Great Goblin\\', \\'Thror\\', \\'Thrain\\', \\'Thranduil\\', \\'Azog\\', \\'Bolg\\', \\'Yazneg\\', \\'Master Worrywort\\', \\'Lindir\\', \\'Goblin Scribe\\', \\'Necromancer\\', \\'Dwarf Miner\\', \\'Young Thrain\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Little Bilbo\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Cute Young Hobbit\\', \\'Moria Orc\\',\\nCharacter Names: [\\'Alex\\', \\'Mr Alexander\\', \\'Chief Guard\\', \\'Dim\\', \\'Stage Actor\\', \\'Mrs. Alexander\\', \\'Dr. Brodsky\\', \\'Tramp\\', \\'Joe the Lodger\\', \\'Prison Governor\\', \\'Catlady\\', \\'Georgie\\', \\'Deltoid\\', \\'Prison Chaplain\\', \\'Mum\\', \\'Dr. Branom\\', \\'Conspirator\\', \\'Minister\\', \\'Dad\\', \\'Psychiatrist\\', \\'Conspirator\\', \\'Det. Const. Tom\\', \\'Police Inspector\\', \\'Pete\\', \\'Julian\\', \\'Dr. Alcott, Ludovico Center check-in\\', \\'Handmaiden in Bible Fantasy\\', \\'Sophisto\\', \"Junior Minister - Minister Frederick\\'s Aid\", \\'Detective sergeant\\', \\'Handmaiden in Bible Fantasy\\', \\'Billyboy\\', \\'Handmaiden in Bible fantasy\\', \\'Nurse Feeley\\', \\'Desk Sergeant\\', \"Victim of Billyboy\\'s Gang\", \\'Sonietta\\', \\'Doctor\\', \\'Nurse\\', \\'Stage Actress\\', \\'Prison Check-in Officer\\', \\'Girl in Ascot Fantasy\\', \\'Mustachioed Ludovico Technician\\', \\'Smiley Prisoner Next to\\nCharacter Names: [\\'Lady Jessica\\', \"The Baron\\'s Doctor\", \\'Piter De Vries\\', \\'Padishah Emperor Shaddam IV\\', \\'Shadout Mapes\\', \\'Thufir Hawat\\', \\'Duncan Idaho\\', \\'Paul Atreides\\', \\'Princess Irulan\\', \\'Reverend Mother Ramallo\\', \\'Stilgar\\', \\'Baron Vladimir Harkonnen\\', \\'Nefud\\', \\'Reverend Mother Gaius Helen Mohiam\\', \\'Duke Leto Atreides\\', \\'The Beast Rabban\\', \\'Gurney Halleck\\', \\'Feyd Rautha\\', \\'Doctor Wellington Yueh\\', \\'Doctor Kynes\\', \\'Alia\\', \\'Chani\\', \\'Orlop\\', \\'Otheym\\', \\'Jamis\\', \\'Harah\\', \\'Bene Gesserit Sister\\', \\'Little Fremen Boy\\', \\'Fremen\\', \\'Guard, House Atreides\\', \\'Czigo\\', \\'Bit Part\\', \\'Imperial General\\', \"Harkonnen\\'s Victim\", \\'Spice Worker\\', \\'Fremen Boy\\', \\'Kinet\\', \\'Palace Maid\\', \\'Elderly man\\', \\'Narrator: TV version\\', \\'Fremen\\', \\'Fremen Girl\\', \\'Palace Maid\\', \\'Fedaykin Fighter\\', \"Lady Jessica\\'s Maid\", \\'Bit\\nChiseller\\', \\'Prisoner\\', \\'Prisoner\\', \\'Prisoner\\', \\'Prisoner\\', \\'Hy-Brasilian\\', \\'Prisoner\\', \\'Odin\\', \\'Thor\\', \\'Horribly Slain Warrior\\', \\'Even More Horribly Slain Warrior\\', \"Sven\\'s Grandfather\", \\'Child God\\']\\n    \\nHuman: Question: What is the name of the Dark Lord?\\nHuman: \\nResponse:  I don\\'t know.', additional_kwargs={}, response_metadata={})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"\n# Update `loaded_memory` after response\nmemory.save_context(inputs={'question':question_data['question']},outputs={'response':response})  # Update loaded_memory with the latest system message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-28T18:03:41.471346Z","iopub.execute_input":"2024-10-28T18:03:41.472198Z","iopub.status.idle":"2024-10-28T18:03:41.891589Z","shell.execute_reply.started":"2024-10-28T18:03:41.472155Z","shell.execute_reply":"2024-10-28T18:03:41.890096Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/facebook/bart-large-cnn","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Update `loaded_memory` after response\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquestion_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update loaded_memory with the latest system message\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/memory/summary.py:126\u001b[0m, in \u001b[0;36mConversationSummaryMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_new_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/memory/summary.py:45\u001b[0m, in \u001b[0;36mSummarizerMixin.predict_new_summary\u001b[0;34m(self, messages, existing_summary)\u001b[0m\n\u001b[1;32m     38\u001b[0m new_lines \u001b[38;5;241m=\u001b[39m get_buffer_string(\n\u001b[1;32m     39\u001b[0m     messages,\n\u001b[1;32m     40\u001b[0m     human_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuman_prefix,\n\u001b[1;32m     41\u001b[0m     ai_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_prefix,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexisting_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_lines\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[1;32m    949\u001b[0m     ]\n\u001b[0;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 779\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1502\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1501\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1502\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1504\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1505\u001b[0m     )\n\u001b[1;32m   1506\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:288\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n","\u001b[0;31mBadRequestError\u001b[0m: (Request ID: LVYj7JjsQYgnUTy8pMY3W)\n\nBad request:\nThe following `model_kwargs` are not used by the model: ['watermark', 'stop', 'stop_sequences', 'return_full_text'] (note: typos in the generate arguments will also show up in this list)"],"ename":"BadRequestError","evalue":"(Request ID: LVYj7JjsQYgnUTy8pMY3W)\n\nBad request:\nThe following `model_kwargs` are not used by the model: ['watermark', 'stop', 'stop_sequences', 'return_full_text'] (note: typos in the generate arguments will also show up in this list)","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"# from langchain.chains.combine_documents import StuffDocumentsChain\n# # Define document combination chain (e.g., StuffDocumentsChain)\n# combine_docs_chain = StuffDocumentsChain(llm=llm, prompt=chat_prompt_template)\n\n# # Define a question generator (rephrases question based on chat history)\n# question_generator = StuffDocumentsChain(llm=llm, prompt=chat_prompt_template)\n\n# # Initialize the ConversationalRetrievalChain\n# conversation_chain = ConversationalRetrievalChain(\n#     retriever=ensemble_retriever,\n#     combine_docs_chain=combine_docs_chain,\n#     question_generator=question_generator,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:19:05.456783Z","iopub.status.idle":"2024-10-25T23:19:05.457289Z","shell.execute_reply.started":"2024-10-25T23:19:05.457019Z","shell.execute_reply":"2024-10-25T23:19:05.457047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.chains.combine_documents import create_stuff_documents_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:19:05.458587Z","iopub.status.idle":"2024-10-25T23:19:05.459248Z","shell.execute_reply.started":"2024-10-25T23:19:05.458977Z","shell.execute_reply":"2024-10-25T23:19:05.459003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.chains import ConversationalRetrievalChain\n# # Initialize the conversational retrieval chain\n# conversation_chain = ConversationalRetrievalChain.from_llm(\n#     retriever=ensemble_retriever,  \n#     llm=llm, \n#     memory=memory,\n#     combine_docs_chain_kwargs={'prompt': chat_prompt_template},\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:26:27.205503Z","iopub.execute_input":"2024-10-25T23:26:27.205873Z","iopub.status.idle":"2024-10-25T23:26:27.211767Z","shell.execute_reply.started":"2024-10-25T23:26:27.205837Z","shell.execute_reply":"2024-10-25T23:26:27.210875Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from langchain.chains import (\n                create_history_aware_retriever,\n                create_retrieval_chain,\n            )\nhistory_aware_retriever = create_history_aware_retriever(\n                llm, ensemble_retriever, chat_prompt_template\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain.chains import ConversationalRetrievalChain\n# from langchain.chains.combine_documents import StuffDocumentsChain\n\n# # Initialize a document combination chain with your prompt template\n# combine_docs_chain = StuffDocumentsChain(llm=llm, prompt=chat_prompt_template)\n\n# # Initialize the conversational retrieval chain using `combine_docs_chain`\n# conversation_chain = ConversationalRetrievalChain.from_llm(\n#     retriever=ensemble_retriever,\n#     llm=llm,\n#     memory=memory,\n#     combine_docs_chain=combine_docs_chain  # Use the combine_docs_chain directly\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-25T23:33:30.665637Z","iopub.execute_input":"2024-10-25T23:33:30.666554Z","iopub.status.idle":"2024-10-25T23:33:30.715120Z","shell.execute_reply.started":"2024-10-25T23:33:30.666511Z","shell.execute_reply":"2024-10-25T23:33:30.714062Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StuffDocumentsChain\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize a document combination chain with your prompt template\u001b[39;00m\n\u001b[1;32m      5\u001b[0m combine_docs_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mchat_prompt_template)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'StuffDocumentsChain' from 'langchain.chains.combine_documents' (/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'StuffDocumentsChain' from 'langchain.chains.combine_documents' (/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/__init__.py)","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"from langchain.chains import (\n                create_history_aware_retriever,\n                create_retrieval_chain,\n            )\n            from langchain.chains.combine_documents import create_stuff_documents_chain\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n            from langchain_openai import ChatOpenAI\n\n\n            retriever = ...  # Your retriever\n\n            llm = ChatOpenAI()\n\n            # Contextualize question\n            contextualize_q_system_prompt = (\n                \"Given a chat history and the latest user question \"\n                \"which might reference context in the chat history, \"\n                \"formulate a standalone question which can be understood \"\n                \"without the chat history. Do NOT answer the question, just \"\n                \"reformulate it if needed and otherwise return it as is.\"\n            )\n            contextualize_q_prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", contextualize_q_system_prompt),\n                    MessagesPlaceholder(\"chat_history\"),\n                    (\"human\", \"{input}\"),\n                ]\n            )\n            history_aware_retriever = create_history_aware_retriever(\n                llm, retriever, contextualize_q_prompt\n            )\n\n            # Answer question\n            qa_system_prompt = (\n                \"You are an assistant for question-answering tasks. Use \"\n                \"the following pieces of retrieved context to answer the \"\n                \"question. If you don't know the answer, just say that you \"\n                \"don't know. Use three sentences maximum and keep the answer \"\n                \"concise.\"\n                \"\\n\\n\"\n                \"{context}\"\n            )\n            qa_prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", qa_system_prompt),\n                    MessagesPlaceholder(\"chat_history\"),\n                    (\"human\", \"{input}\"),\n                ]\n            )\n            # Below we use create_stuff_documents_chain to feed all retrieved context\n            # into the LLM. Note that we can also use StuffDocumentsChain and other\n            # instances of BaseCombineDocumentsChain.\n            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n            rag_chain = create_retrieval_chain(\n                history_aware_retriever, question_answer_chain\n            )\n\n            # Usage:\n            chat_history = []  # Collect chat history here (a sequence of messages)\n            rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n\n    This chain takes in chat history (a list of messages) and new questions,\n    and then returns an answer to that question.\n    The algorithm for this chain consists of three parts:\n\n    1. Use the chat history and the new question to create a \"standalone question\".\n    This is done so that this question can be passed into the retrieval step to fetch\n    relevant documents. If only the new question was passed in, then relevant context\n    may be lacking. If the whole conversation was passed into retrieval, there may\n    be unnecessary information there that would distract from retrieval.\n\n    2. This new question is passed to the retriever and relevant documents are\n    returned.\n\n    3. The retrieved documents are passed to an LLM along with either the new question\n    (default behavior) or the original question and chat history to generate a final\n    response.\n\n    Example:\n        .. code-block:: python\n\n            from langchain.chains import (\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n            )\n            from langchain_core.prompts import PromptTemplate\n            from langchain_community.llms import OpenAI\n\n            combine_docs_chain = StuffDocumentsChain(...)\n            vectorstore = ...\n            retriever = vectorstore.as_retriever()\n\n            # This controls how the standalone question is generated.\n            # Should take `chat_history` and `question` as input variables.\n            template = (\n                \"Combine the chat history and follow up question into \"\n                \"a standalone question. Chat History: {chat_history}\"\n                \"Follow up question: {question}\"\n            )\n            prompt = PromptTemplate.from_template(template)\n            llm = OpenAI()\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n            chain = ConversationalRetrievalChain(\n                combine_docs_chain=combine_docs_chain,\n                retriever=retriever,\n                question_generator=question_generator_chain,\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from langchain import LLMChain\n\n# llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the entire dataset\ntest_file_path = hf_hub_download(repo_id=\"niloufarna/MovieChat\", subfolder='dataset', filename=\"combined_movie_dataset.json\", local_dir= '.', repo_type=\"dataset\")\ndataset = load_dataset('json', data_files = test_file_path, split = 'train')\n# Filter each split based on the 'split' field\n\ntest_dataset = dataset['train'].filter(lambda x: x['split'] == 'test')\n\n\n# Example usage\n\nprint(\"Test Dataset:\", test_dataset)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generation = rag_chain.invoke({\"context\": ensemble_relevant_docs, \"question\": question})\nprint(generation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nresponse = llm_chain.run(question)\n\nprint(response)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset1[7]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"Find a movie similar to Inception\"\nquery_embedding = embedding_function.embed_query(query)\nsearch_results = chroma_index.similarity_search(query_embedding)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"documents","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset1.features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset2.features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset2[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf1 = pd.read_csv(file_path1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df1 = df1.dropna(subset=['synopsis','plot','plot_outline'], how='all')\ndf1['synopsis'] = df1['synopsis'].fillna(df1['plot_outline'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = pd.read_csv(file_path2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2[df2['plot_outline'].isna()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1[df1['votes'].isna()]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
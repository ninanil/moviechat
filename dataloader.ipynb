{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9644810,"sourceType":"datasetVersion","datasetId":5818808},{"sourceId":201373721,"sourceType":"kernelVersion"}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import display\nimport logging\nfrom sklearn.model_selection import train_test_split\nfrom transformers import logging as transformers_logging\n\n# Set transformers logging level early\ntransformers_logging.set_verbosity_warning()\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\ntransformers_logger.propagate = False\n\nclass DisplayHandler(logging.Handler):\n    def emit(self, record):\n        display(self.format(record))\n\n# Configure logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Add custom display handler\ndisplay_handler = DisplayHandler()\ndisplay_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s:%(message)s'))\nlogger.addHandler(display_handler)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:37:45.837731Z","iopub.execute_input":"2024-10-16T21:37:45.838149Z","iopub.status.idle":"2024-10-16T21:37:51.162575Z","shell.execute_reply.started":"2024-10-16T21:37:45.838106Z","shell.execute_reply":"2024-10-16T21:37:51.161275Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install hydra-core omegaconf wandb\n\n\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:37:51.164977Z","iopub.execute_input":"2024-10-16T21:37:51.165677Z","iopub.status.idle":"2024-10-16T21:38:12.546512Z","shell.execute_reply.started":"2024-10-16T21:37:51.165618Z","shell.execute_reply":"2024-10-16T21:38:12.545243Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting hydra-core\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting omegaconf\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.1)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core) (21.3)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf) (6.0.2)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.14.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->hydra-core) (3.1.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=20832a4afb697ecb1d52ce27447f37875aeedb004050190fe43e2ea77678b9c6\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, omegaconf, hydra-core\nSuccessfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# import hydra\n# from omegaconf import DictConfig\n\n# #@hydra.main(config_name = \"config\", config_path=\"/kaggle/input/config\")\n# def my_app(cfg: DictConfig):\n#     repo_id = cfg.huggingface.repo_id\n#     hf_api_token = cfg.huggingface.api_token\n#     wandb_api_token = cfg.wandb.api_token\n#     wandb_project_name = cfg.wandb.project_name\n#     wandb_entity = cfg.wandb.entity\n#     return repo_id, hf_api_token, wandb_api_token, wandb_project_name, wandb_entity\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:38:12.548286Z","iopub.execute_input":"2024-10-16T21:38:12.548782Z","iopub.status.idle":"2024-10-16T21:38:12.555861Z","shell.execute_reply.started":"2024-10-16T21:38:12.548727Z","shell.execute_reply":"2024-10-16T21:38:12.554623Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login, snapshot_download, HfApi\nimport hydra\nfrom dotenv import load_dotenv\nfrom omegaconf import OmegaConf","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:38:12.558506Z","iopub.execute_input":"2024-10-16T21:38:12.558903Z","iopub.status.idle":"2024-10-16T21:38:12.757714Z","shell.execute_reply.started":"2024-10-16T21:38:12.558862Z","shell.execute_reply":"2024-10-16T21:38:12.756539Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from abc import ABC, abstractmethod\nimport os\nimport pandas as pd\nimport re\nimport json, random\nimport logging\nfrom structured_configs import CornellDatasetConfig, MovieQADatasetConfig\n# Configure logging\n# logging.basicConfig(level=logging.INFO)\n# logger = logging.getLogger(__name__)\n# Configure logging\n\n# logging.basicConfig(filename='example.log', \n#                     encoding='utf-8', \n#                     level=logging.DEBUG,\n#                     format=\"%(asctime)s:%(levelname)s:%(message)s\"\n#                    )\n\n# # Create a logger object\n# logger = logging.getLogger()\n\n\nclass BaseDataLoader(ABC):\n    \"\"\"\n    Abstract base class for data loaders.\n    \"\"\"\n    \n    def __init__(self):\n        pass\n    \n    @abstractmethod\n    def load_data(self):\n        \"\"\"\n        Load data from the specified source.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def preprocess_data(self):\n        \"\"\"\n        Preprocess the loaded data.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def train_val_test_split(self):\n        pass\n    \n    @abstractmethod\n    def merge_dataframes(self):\n        pass\n\n    @abstractmethod\n    def convert_to_json(self):\n        \"\"\"\n        Convert the processed data to JSON format.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_data(self):\n        \"\"\"\n        Save the JSON data to a file.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def concat_json_files(file_path1, file_path2):\n        try:\n            # Open and read the first JSON file\n            with open(file_path1, \"r\") as file1:\n                data1 = json.load(file1)\n            logging.info(f\"Successfully loaded data from {file_path1}\")\n\n            # Open and read the second JSON file\n            with open(file_path2, \"r\") as file2:\n                data2 = json.load(file2)\n            logging.info(f\"Successfully loaded data from {file_path2}\")\n\n            # Concatenate the data from both files\n            data = data1 + data2\n            logging.info(f\"Successfully concatenated data from {file_path1} and {file_path2}\")\n            full_output_path = os.path.join(\".\",cfg.hf.combined_dataset.file_name)\n            with open(full_output_path, 'w', encoding='utf-8') as file:\n                    json.dump(data, file, ensure_ascii=False, indent=4)\n            if cfg.hf.combined_dataset.to_hf:\n                api.upload_file(\n                path_or_fileobj=cfg.hf.combined_dataset.file_name,\n                repo_id= cfg.hf.repo_id,\n                path_in_repo = f\"{cfg.hf.combined_dataset.path_in_repo}/{cfg.hf.combined_dataset.file_name}\",\n                repo_type=\"dataset\",\n                commit_message=cfg.hf.combined_dataset.commit_message,\n                commit_description=cfg.hf.combined_dataset.commit_description\n                    )\n                logger.info(f\"File {cfg.hf.combined_dataset.file_name}  logged to Huggingface successfully.\")\n                if cfg.wandb.combined_dataset.to_wandb:\n                    artifact = wandb.Artifact(name=cfg.wandb.combined_dataset.json_artifact_name, \n                                          description= cfg.wandb.combined_dataset.description, type='dataset')  # Name and type for the artifact\n                    artifact.add_file(full_output_path)  # Add the saved JSON file to the artifact\n                    #wandb.log_artifact(artifact)  # Log the artifact to WandB\n                    artifact.save()\n                    logger.info(f\"File {cfg.hf.combined_dataset.file_name}  logged to WandB successfully.\")\n        except FileNotFoundError as e:\n            logging.error(f\"File not found: {e}\")\n            raise\n\n        except json.JSONDecodeError as e:\n            logging.error(f\"Error decoding JSON: {e}\")\n            raise\n\n        except Exception as e:\n            logging.error(f\"An unexpected error occurred: {e}\")\n            raise\n    \nclass CornellDataLoader(BaseDataLoader):\n    \"\"\"\n    Concrete data loader for Cornell movie datasets.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initialize the CornellDataLoader.\n\n        :param folder_path: Path to the folder containing data files.\n        :param data_path_list: List of data file paths.\n        :param column_names_list: List of column names for each data file.\n        :param output_path: Path to save the output JSON file.\n        :param output_path: Path to save the output JSON file.\n        :param sample_fraction: Fraction of new samples to be generated.\n        :param train_ratio: Proportion of the data used for training, default is 0.9 (90%).\n        :param test_ratio: Proportion of the data used for testing, default is 0.05 (5%).\n        :param generate_new_questions: Boolean flag to indicate whether new questions should be generated.\n        \n        \"\"\"\n        self.config = config\n        self.movie_dataset_df = pd.DataFrame()\n        self.movie_dataset = []\n        self.metadata_questions = pd.DataFrame()\n        \n    def load_data(self):\n        \"\"\"\n        Load dataset files into pandas DataFrames.\n\n        :return: List of pandas DataFrames.\n        \"\"\"\n        try:\n            movie_df_list = [self._read_csv_file(file_key) for file_key in self.config.files ]\n            logger.info(\"Data loaded successfully.\")\n            return movie_df_list\n        except Exception as e:\n            logger.error(f\"Error loading data: {e}\")\n            raise\n\n    def _read_csv_file(self, file_kay):\n        \"\"\"\n        Read a single CSV file into a pandas DataFrame.\n\n        :return: pandas DataFrame.\n        \"\"\"\n        try:\n            file_config = self.config.files[file_kay]\n            path = os.path.join(self.config.folder_path, file_config['file_name'])\n            column_names = file_config['columns']\n            df = pd.read_csv(path, engine='python', sep = file_config['sep'], \n                             names = column_names, encoding=file_config['encoding'])\n            logger.debug(f\"Loaded data from {path} with columns {column_names}.\")\n            return df\n        except FileNotFoundError:\n            logger.error(f\"File not found: {path}\")\n            raise\n        except pd.errors.ParserError as e:\n            logger.error(f\"Parser error while reading {path}: {e}\")\n            raise\n            \n    def preprocess_data(self, df_list):\n        \"\"\"\n        Preprocesses the given dataframes by cleaning and formatting specific columns.\n\n        \"\"\"\n        try:\n            # Unpack dataframes from the input list\n            movie_conversation_df, movie_utterances_df, movie_metadata_df, characters_df, imdb_details_df = df_list\n\n            # Title case the 'character_name' column in both utterances and characters dataframes\n            movie_utterances_df['character_name'] = movie_utterances_df['character_name'].str.title()\n            characters_df['character_name'] = characters_df['character_name'].str.title()\n\n            # Clean 'release_year' by removing any non-digit characters and converting to integer\n            movie_metadata_df['release_year'] = movie_metadata_df['release_year'].str.replace(r'\\D', '', regex=True)\n            movie_metadata_df['release_year'] = movie_metadata_df['release_year'].astype(int)\n\n            # Extract genres from the 'genre' column and join multiple genres with commas\n            movie_metadata_df['genre'] = movie_metadata_df['genre'].str.extractall(r\"'(.*?)'\")[0].groupby(level=0).apply(', '.join)\n\n            # Round 'imdb_rating' to one decimal place and ensure the type is float\n            movie_metadata_df['imdb_rating'] = movie_metadata_df['imdb_rating'].map(lambda r: round(r, 1)).astype(float)\n\n            # Parse the 'line_id_list' column into actual lists using a helper method\n            movie_conversation_df['line_id_list'] = movie_conversation_df['line_id_list'].apply(self._parse_list_string)\n            \n            \n            imdb_details_df = imdb_details_df[['movie_name', 'plot_outline']]\n            print(\"preprocess imdb_details_df\", imdb_details_df.columns)\n            logger.info(\"Data preprocessing completed successfully.\")\n\n            return movie_conversation_df, movie_utterances_df, movie_metadata_df, characters_df, imdb_details_df\n\n        except Exception as e:\n            logger.error(f\"Error during data preprocessing: {str(e)}\")\n            raise  # Re-raise the exception after logging\n    \n    def _row_to_json(self, row):\n        \"\"\"\n        Convert a DataFrame row to a JSON-like dictionary.\n\n        :param row: pandas Series representing a row in the DataFrame.\n        :return: Dictionary representing the JSON object.\n        \"\"\"\n        \n\n        json_obj = {\n            \"split\": row.get('split', 'train'),\n            \"type\":\"dialogue\",\n            \"instruction\": row.get('instruction', 'Continue the conversation between the characters.'),\n            \"input\": row.get('utterance', ''),\n            \"context\": {\n                \"movie_name\": row.get('movie_name', 'Unknown'),\n                \"character_names\": row.get('character_names', 'Unknown'),\n                \"genre\": row.get('genre', ''),\n                \"year\": row.get('release_year', 'Unknown'),\n                \"imdb_rating\": row.get('imdb_rating', 0),\n                \"num_imdb_votes\": row.get('num_imdb_votes', 0),\n                'plot_outline': row.get('plot_outline', 'Unknown'),\n                \"additional_information\": None\n            },\n            \"response\": row.get('response', 'Unknown')\n        }\n\n        return json_obj\n\n    def convert_to_json(self):\n        \"\"\"\n        Convert the list of DataFrames into a JSON-compatible list of dictionaries.\n\n        :param df_list: List of pandas DataFrames.\n        \"\"\"\n        \n        try:\n            \n            # Convert the main dataset to JSON using the existing method\n            self.movie_dataset = self.movie_dataset_df.apply(self._row_to_json, axis=1).tolist()\n            \n        \n            logger.info(\"Data converted to JSON format successfully.\")\n        except Exception as e:\n            logger.error(f\"Error converting data to JSON: {e}\")\n            raise\n    def _generate_samples(self, row):\n        \"\"\"\n        Generate new samples by modifying the question and response of a given row.\n\n        Args:\n            row (namedtuple): A row from the DataFrame with movie details and dialogue.\n\n        Returns:\n            pd.DataFrame: A DataFrame with the modified question-answer samples.\n        \"\"\"\n        # Define question templates\n        genre_questions = [\n            'What genre is the movie {movie_name}?',\n            'Which genres does {movie_name} belong to?',\n            'Can you tell me the genre of {movie_name}?',\n            'What are the main genres of {movie_name}?',\n            'Under which genres is {movie_name} classified?',\n            'What type of film is {movie_name}?',\n            'What genres categorize {movie_name}?'\n        ]\n\n        release_year_questions = [\n            'In what year was {movie_name} released?',\n            'When did {movie_name} come out?',\n            'What is the release year of {movie_name}?',\n            'Which year did {movie_name} premiere?',\n            'When was {movie_name} first released?',\n            'What year did {movie_name} hit theaters?',\n            'Can you tell me the release year of {movie_name}?'\n        ]\n\n        imdb_rating_questions = [\n            'What is the IMDb rating of {movie_name}?',\n            'How is {movie_name} rated on IMDb?',\n            'Can you tell me the IMDb score for {movie_name}?',\n            'What rating did {movie_name} receive on IMDb?',\n            'What is the IMDb rating for {movie_name}?'\n        ]\n\n        imdb_votes_questions = [\n            'How many votes does {movie_name} have on IMDb?',\n            'What is the total number of IMDb votes for {movie_name}?',\n            'How many people rated {movie_name} on IMDb?',\n            'What is the number of votes for {movie_name} on IMDb?'\n        ]\n        # List to collect new samples\n        new_samples = []\n\n        # Extract values from the row\n        movie_name = getattr(row, 'movie_name', 'Unknown')\n        genre = getattr(row, 'genre', 'Unknown')\n        year = getattr(row, 'release_year', 'Unknown')\n        imdb_rating = getattr(row, 'imdb_rating', 'Unknown')\n        imdb_votes = getattr(row, 'num_imdb_votes', 'Unknown')\n        \n        # Helper function to create a new modified row\n        def add_modified_row(new_question, new_response):\n            # Convert row back to a dictionary and modify question and response\n            new_row = row._asdict()  # Convert namedtuple row to a dictionary\n            new_row['utterance'] = new_question\n            new_row['response'] = new_response\n            new_row['instruction'] = 'Answer the following question:'\n            new_samples.append(new_row)  # Add the modified row to the samples list\n\n        # Modify question and response based on attributes and add the new row to the samples list\n        if genre != 'Unknown':\n            new_question = random.choice(genre_questions).format(movie_name=movie_name)\n            add_modified_row(new_question, genre)\n\n        if year != 'Unknown':\n            new_question = random.choice(release_year_questions).format(movie_name=movie_name)\n            add_modified_row(new_question, year)\n\n        if imdb_rating != 'Unknown':\n            new_question = random.choice(imdb_rating_questions).format(movie_name=movie_name)\n            add_modified_row(new_question, imdb_rating)\n\n        if imdb_votes != 'Unknown':\n            new_question = random.choice(imdb_votes_questions).format(movie_name=movie_name)\n            add_modified_row(new_question, imdb_votes)\n\n        return new_samples\n\n    def _parse_list_string(self, value):\n        \"\"\"\n        Parse a string representation of a list into an actual list.\n\n        :param value: String to parse.\n        :return: List of extracted strings.\n        \"\"\"\n        clean_string = re.findall(r\"'(.*?)'\", value)\n\n        if clean_string:\n            return clean_string\n        else:\n            logger.warning(f\"Could not parse: {value}\")\n            return []\n\n    def merge_dataframes(self, df_list):\n        \"\"\"\n        Merge multiple DataFrames into a single DataFrame for processing.\n\n        :param df_list: List of pandas DataFrames.\n        :return: Merged pandas DataFrame.\n        \"\"\"\n        try:\n            if len(df_list) != 5:\n                raise ValueError(\"df_list must contain exactly four DataFrames.\")\n\n            movie_conversation_df, movie_utterances_df, movie_metadata_df, characters_df, imdb_details_df = df_list\n\n            # Concatenate character name with utterance\n            movie_utterances_df['utterance'] = movie_utterances_df['character_name'].str.cat(\n                movie_utterances_df['utterance'], sep=': '\n            )\n            \n            # Merge conversation and metadata DataFrames on 'movie_id'\n            movie_conversation_metadata_df = pd.merge(\n                left=movie_conversation_df,\n                right=movie_metadata_df,\n                on='movie_id',\n                how='inner'\n            )\n            \n            # Merge with character details\n            movie_conversation_metadata_df = pd.merge(\n                left=movie_conversation_metadata_df,\n                right=characters_df,\n                left_on='character_id1',\n                right_on='character_id',\n                how='inner'\n            )\n            \n            \n            # Drop redundant columns\n            movie_conversation_metadata_df.drop(\n                columns=['character_id1', 'movie_id_x', 'movie_id_y', 'character_id', 'gender', 'position'],\n                inplace=True\n            )\n            \n            # Map character IDs to names\n            characters_dict = characters_df.set_index('character_id')['character_name'].to_dict()\n            movie_conversation_metadata_df['character_name2'] = movie_conversation_metadata_df['character_id2'].map(characters_dict)\n\n            # Rename columns for clarity\n            movie_conversation_metadata_df.rename(\n                columns={'character_name': 'character_name1', 'movie_name_x': 'movie_name'},\n                inplace=True\n            )\n            movie_conversation_metadata_df.drop(columns=['character_id2'], inplace=True)\n           \n\n            # Save the original DataFrame index for grouping\n            movie_conversation_metadata_df['original_line_index'] = movie_conversation_metadata_df.index\n\n            # Explode the 'line_id_list' to have one row per line ID\n            movie_conversation_metadata_df = movie_conversation_metadata_df.explode('line_id_list')\n\n            # Merge with utterances DataFrame using 'line_id_list' as the key\n            movie_conversation_metadata_df = pd.merge(\n                left=movie_conversation_metadata_df,\n                right=movie_utterances_df,\n                left_on='line_id_list',\n                right_index=True,\n                how='inner'\n            )\n\n            # Concatenate character names\n            movie_conversation_metadata_df['character_names'] = (\n                movie_conversation_metadata_df['character_name1'] + ', ' +\n                movie_conversation_metadata_df['character_name2']\n            )\n            # Create 'response' column as a copy of 'utterance'\n            movie_conversation_metadata_df['response'] = movie_conversation_metadata_df['utterance'].copy(deep=True)\n            \n            # Aggregate the data\n            movie_dataset_df = movie_conversation_metadata_df.groupby('original_line_index').agg({\n                'character_names': 'first',\n                'movie_name': 'first',\n                'release_year': 'first',\n                'imdb_rating': 'mean',\n                'num_imdb_votes': 'first',\n                'genre': 'first',\n                'utterance': lambda u: '\\n '.join(map(str, u[:-1])) if len(u) > 1 else u.iloc[0],\n                'response': 'last'\n            }).reset_index().drop(columns=['original_line_index'])\n            \n            # print(\"in merge_dataframe imdb_details_df\", imdb_details_df.columns, imdb_details_df.size)\n            # print(\"imdb_details_df\", imdb_details_df.head(5))\n            movie_dataset_df = pd.merge(left = movie_dataset_df, right = imdb_details_df, on = 'movie_name', how='left')\n            # print(\"movie_dataset_df\", movie_dataset_df.head(5), movie_dataset_df.size)\n            # Remove duplicate rows based on all columns\n            movie_dataset_df = movie_dataset_df.drop_duplicates()\n            # Remove rows where 'utterance' or 'response' are NaN\n            self.movie_dataset_df = movie_dataset_df.dropna(subset=['utterance', 'response'])\n            self.movie_dataset_df.loc[:, 'instruction'] = 'Continue the conversation between the characters.'\n            logger.debug(\"DataFrames merged successfully.\")\n            # Generate new samples if flagged\n            if self.config.generate_new_questions:\n                # Sample a fraction of the data\n                sampled_df = self.movie_dataset_df.groupby('movie_name').agg({'character_names':'first','movie_name':'first','genre':'first', 'release_year':'first','imdb_rating':'mean', 'num_imdb_votes':'first'}).sample(frac=self.config.sample_fraction, random_state=42).reset_index(drop=True)\n                logger.info(f\"Sampled {len(sampled_df)} data successfully.\")\n\n                # Generate new samples from the sampled DataFrame\n                #new_sample_list = sampled_df.apply(self._generate_json_sample, axis=1).tolist()\n                new_sample_list = [sample for row in sampled_df.itertuples(index=False) \n                                            for sample in self._generate_samples(row)]\n                new_sample_df = pd.DataFrame(new_sample_list)\n                \n              \n            self.movie_dataset_df = pd.concat([self.movie_dataset_df, new_sample_df], axis = 0).reset_index(drop = True)\n            self.movie_dataset_df['response'] = self.movie_dataset_df['response'].astype(str)    \n            if self.config.data_prune_enabled:\n                self._data_pruning()\n            self._save_df()\n            logger.debug(f\"Total number of rows found in movie_dataset_df: {len(self.movie_dataset_df)}\")\n            return self.movie_dataset_df\n        except Exception as e:\n            logger.error(f\"Error merging DataFrames: {e}\")\n            raise\n    def _data_pruning(self):\n\n        logger.info(f\"Cornell dataset size before pruning: {self.movie_dataset_df.shape[0]} rows\")\n        filtered_movies = self.movie_dataset_df.groupby('movie_name').size()\n        filtered_movies = filtered_movies[filtered_movies > self.config.frequent_sample_ratio].index\n\n        # Filter the main dataset to include only those movies\n        self.movie_dataset_df = self.movie_dataset_df[self.movie_dataset_df['movie_name'].isin(filtered_movies)].reset_index(drop = True)\n        logger.info(f\"Cornell dataset size after pruning: {self.movie_dataset_df.shape[0]} rows\")\n        \n    def train_val_test_split(self):\n        # Filtering movies that appear more than 50 times\n        # filtered_movies = self.movie_dataset_df.groupby('movie_name').size()\n        # filtered_movies = filtered_movies[filtered_movies > 50].index\n\n        # Filter the main dataset to include only those movies\n        # filtered_dataset = self.movie_dataset_df[self.movie_dataset_df['movie_name'].isin(filtered_movies)]\n\n        # Calculating sample sizes for training, validation, and test\n        total_records = len(self.movie_dataset_df)\n        train_size = int(total_records * self.config.train_ratio)  # 90% training\n        test_size = int(total_records * self.config.test_ratio)    # 0.05% test\n        val_size = total_records - train_size - test_size  # remaining for validation\n        \n        # Split the data into train, validation, and test\n        train_data, temp_data = train_test_split(\n            self.movie_dataset_df, \n            train_size=train_size, \n            stratify=self.movie_dataset_df['movie_name'], \n            random_state=42\n        )\n\n        try:\n             test_data,val_data = train_test_split(\n                temp_data, \n                train_size=test_size, \n                stratify=temp_data['movie_name'], \n                random_state=42\n            )\n        except ValueError as e:\n            logger.warning(f\"Stratified split failed: {e}. Falling back to non-stratified split.\")\n            test_data,val_data = train_test_split(\n                temp_data, \n                train_size=test_size, \n                random_state=42\n            )\n        # Assign the split column\n        train_data['split'] = 'train'\n        val_data['split'] = 'val'\n        test_data['split'] = 'test'\n\n        # Combine the data back into a single dataset\n        self.movie_dataset_df = pd.concat([train_data, val_data, test_data], ignore_index=True)\n\n        # Logging the size of each dataset\n        logger.info(f\"Training set size: {len(train_data)}\")\n        logger.info(f\"Validation set size: {len(val_data)}\")\n        logger.info(f\"Test set size: {len(test_data)}\")\n        \n        return self.movie_dataset_df\n\n    def save_data(self):\n        \"\"\"\n        Save the converted JSON data to the specified output file.\n        \"\"\"\n        try:\n            full_output_path  = os.path.join(self.config.folder_path, self.config.output_path)\n            with open(full_output_path , 'w', encoding='utf-8') as f:\n                json.dump(self.movie_dataset, f, ensure_ascii=False, indent=4)\n            logger.info(f\"Data saved to {full_output_path }\")\n            if cfg.wandb.cornell_dataset.to_wandb:\n                artifact = wandb.Artifact(name=cfg.wandb.cornell_dataset.json_artifact_name,\n                                          description = cfg.wandb.cornell_dataset.description,type='dataset')  # Name and type for the artifact\n                artifact.add_file(full_output_path)  # Add the saved JSON file to the artifact\n    #             wandb.log_artifact(artifact)  # Log the artifact to WandB\n                artifact.save()\n                logger.info(f\"JSON file '{self.config.output_path}' uploaded to WandB successfully.\")\n            if cfg.hf.cornell_dataset.to_hf:\n                 api.upload_file(\n                    path_or_fileobj=full_output_path,\n                    repo_id=cfg.hf.repo_id,\n                    path_in_repo=f\"{cfg.hf.cornell_dataset.path_in_repo}{cfg.hf.cornell_dataset.file_name}\",\n                    repo_type=\"dataset\")\n                 logger.info(f\"File {self.config.output_path}  logged to Huggingface successfully.\")\n            return full_output_path\n        except Exception as e:\n            logger.error(f\"Error saving data: {e}\")\n            raise\n    \n    def _save_df(self):\n        \"\"\"\n        Save the converted JSON data to the specified output file.\n        \"\"\"\n        try:\n            output_path = os.path.join(self.config.folder_path,'movie_dataset.csv')\n            # Save the DataFrame to a CSV file\n            self.movie_dataset_df.to_csv(output_path, index=False)\n            logger.info(f\"DataFrame saved as CSV file at '{output_path}'.\")\n            if cfg.wandb.cornell_dataset.to_wandb:\n                # Create a WandB artifact\n                artifact = wandb.Artifact(name=cfg.wandb.cornell_dataset.csv_artifact_name, type= 'dataset',\n                                          description = cfg.wandb.cornell_dataset.description,\n                                         metadata = {'size':len(self.movie_dataset_df),\n                                                    'columns': self.movie_dataset_df.columns.to_list()\n    })\n    \n                # Add the saved CSV file to the artifact\n                artifact.add_file(output_path)\n                logger.info(f\"CSV file '{output_path}' added to WandB artifact cornell_movie_df.\")\n    \n                # Save the artifact\n                wandb.log_artifact(artifact)\n                logger.info(f\"Artifact cornell_movie_df logged to WandB successfully.\")\n\n        except Exception as e:\n            logger.error(f\"Error saving DataFrame or uploading to WandB: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T21:50:03.402551Z","iopub.execute_input":"2024-10-16T21:50:03.403868Z","iopub.status.idle":"2024-10-16T21:50:03.499757Z","shell.execute_reply.started":"2024-10-16T21:50:03.403799Z","shell.execute_reply":"2024-10-16T21:50:03.498318Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class MovieQADataLoader(BaseDataLoader):\n    def __init__(self, config):\n        self.config = config\n        self.qa_dataset = []\n        self.qa_movie_metadata_df = pd.DataFrame()\n        \n    def load_data(self):\n        \"\"\"\n        Load Q&A dataset.\n        \"\"\"\n        try:\n            qa_df_list = []\n            for file,file_config in self.config.files.items():\n                path = os.path.join(self.config.folder_path,file_config['file_name'])\n                if path.endswith('.json'):\n                    qa_df_list.append(self._read_json_file(path))\n                else:\n                    qa_df_list.append(self._read_csv_file(file_config))\n            \n            logger.info(\"Data loaded successfully.\")\n            return qa_df_list\n        except Exception as e:\n            logger.error(f\"Error loading data: {e}\")\n            raise\n    def _read_json_file(self, data_path):\n        try:\n            qa_df = pd.read_json(data_path)\n            logger.info(\"Q&A JSON file loaded successfully.\")\n            return qa_df\n        except Exception as e:\n            logger.error(f\"Error loading Q&A JSON file: {e}\")\n            raise\n    def _read_csv_file(self, file):\n        try:\n            path = os.path.join(self.config.folder_path, file['file_name'])\n            qa_df = pd.read_csv(path, usecols=file['columns'], \n                                sep = file['sep'], encoding=file['encoding'])\n            logger.info(\"Q&A CSV file loaded successfully.\")\n            return qa_df\n        except Exception as e:\n            logger.error(f\"Error loading Q&A JSON file: {e}\")\n            raise\n    def preprocess_data(self,df_list):\n        qa_df, movie_df, imdb_details_df = df_list\n        movie_df = movie_df[['genre', 'imdb_key', 'name', 'year']]\n        qa_df = qa_df[qa_df['correct_index'].notna()]\n        qa_df['response'] = qa_df.apply(lambda row: row['answers'][int(row['correct_index'])], axis=1)\n        qa_df = qa_df[['question','imdb_key' ,'response']]\n        return qa_df, movie_df, imdb_details_df\n    def merge_dataframes(self, df_list): \n        qa_df, movie_df, imdb_details_df = df_list\n        self.qa_movie_metadata_df = pd.merge(left = qa_df, right = movie_df, on = 'imdb_key', how='left')\n        self.qa_movie_metadata_df = pd.merge(left = self.qa_movie_metadata_df , right = imdb_details_df, left_on = 'name', right_on = 'title', how= 'left').drop(columns=['title']).rename(columns={'name':'movie_name'})\n        self.qa_movie_metadata_df.drop(columns=['imdb_key'], inplace = True)\n        self._save_df()\n        return self.qa_movie_metadata_df\n    def convert_to_json(self):\n        self.qa_dataset = self.qa_movie_metadata_df.apply(self._row_to_json, axis=1).tolist()\n        return self.qa_dataset\n    def _row_to_json(self, row):\n        return {\n            \"split\": row.get('split', 'train'),\n            \"type\":\"qa\",\n            \"instruction\": \"Answer the following question:\",\n            \"input\": row['question'],\n            \"context\": {\n                \"movie_name\": row.get('movie_name', 'Unknown'),\n                \"genre\": row.get('genre', 'Unknown'),\n                \"year\": row.get('year', 'Unknown'),\n                \"plot_outline\": row.get('plot_outline', 'Unknown'),\n                \"additional_information\": None\n            },\n            \"response\": row['response']\n        }\n    def train_val_test_split(self):\n        # Filtering movies that appear more than 50 times\n        filtered_qa_movies = self.qa_movie_metadata_df.groupby('movie_name').size()\n        filtered_qa_movies = filtered_qa_movies[filtered_qa_movies > 50].index\n\n        # Filter the main dataset to include only those movies\n        filtered_dataset = self.qa_movie_metadata_df[self.qa_movie_metadata_df['movie_name'].isin(filtered_qa_movies)]\n\n        # Calculating sample sizes for training, validation, and test\n        total_records = len(filtered_dataset)\n        train_size = int(total_records * self.config.train_ratio)  # 90% training\n        test_size = int(total_records * self.config.test_ratio)    # 0.05% test\n        val_size = total_records - train_size - test_size  # remaining for validation\n\n        # Split the data into train, validation, and test\n        train_data, temp_data = train_test_split(\n            filtered_dataset, \n            train_size=train_size, \n            stratify=filtered_dataset['movie_name'], \n            random_state=42\n        )\n\n        try:\n             test_data,val_data = train_test_split(\n                temp_data, \n                train_size=test_size, \n                stratify=temp_data['movie_name'], \n                random_state=42\n            )\n        except ValueError as e:\n            logger.warning(f\"Stratified split failed: {e}. Falling back to non-stratified split.\")\n            test_data,val_data = train_test_split(\n                temp_data, \n                train_size=test_size, \n                random_state=42\n            )\n\n        # Assign the split column\n        train_data['split'] = 'train'\n        val_data['split'] = 'val'\n        test_data['split'] = 'test'\n\n        # Combine the data back into a single dataset\n        self.qa_movie_metadata_df = pd.concat([train_data, val_data, test_data], ignore_index=True)\n\n        # Logging the size of each dataset\n        logger.info(f\"Training set size: {len(train_data)}\")\n        logger.info(f\"Validation set size: {len(val_data)}\")\n        logger.info(f\"Test set size: {len(test_data)}\")\n        return self.qa_movie_metadata_df\n    \n    def save_data(self):\n         try:\n            full_output_path = os.path.join(self.config.folder_path ,self.config.output_path)#os.path.join(self.folder_path,self.output_path)\n            with open(full_output_path, 'w', encoding='utf-8') as f:\n                json.dump(self.qa_dataset, f, ensure_ascii=False, indent=4)\n            logger.info(f\"Q&A Movie dataset saved to {self.config.output_path}\")\n            if cfg.wandb.movieqa_dataset.to_wandb:\n                artifact = wandb.Artifact(name=cfg.wandb.movieqa_dataset.json_artifact_name, \n                                          description= cfg.wandb.movieqa_dataset.description, type='dataset')  # Name and type for the artifact\n                artifact.add_file(full_output_path)  # Add the saved JSON file to the artifact\n                #wandb.log_artifact(artifact)  # Log the artifact to WandB\n                artifact.save()\n            if cfg.hf.movieqa_dataset.to_hf:\n                api.upload_file(\n                    path_or_fileobj=full_output_path,\n                    repo_id= cfg.hf.repo_id,\n                    path_in_repo = cfg.hf.movieqa_dataset.path_in_repo,\n                    repo_type=\"dataset\",\n                    )\n                logger.info(f\"Artifact {self.config.output_path}  logged to Huggingface successfully.\")\n            return full_output_path\n         except Exception as e:\n            logger.error(f\"Error saving data: {e}\")\n            raise\n    def _save_df(self):\n        \"\"\"\n        Save the converted JSON data to the specified output file.\n        \"\"\"\n        try:\n            output_path = os.path.join(self.config.folder_path ,'qa_movie_df.csv')\n            # Save the DataFrame to a CSV file\n            self.qa_movie_metadata_df.to_csv(output_path, index=False)\n            logger.info(f\"DataFrame saved as CSV file at '{output_path}'.\")\n            \n            if cfg.wandb.movieqa_dataset.to_wandb:\n                # Create a WandB artifact\n                artifact = wandb.Artifact(name = cfg.wandb.movieqa_dataset.csv_artifact_name, type = 'dataset',\n                                          description= cfg.wandb.movieqa_dataset.description,\n                                        metadata = {'size':len(self.qa_movie_metadata_df),\n                                                    'columns':self.qa_movie_metadata_df.columns.to_list()},\n                                         )\n    \n                # Add the saved CSV file to the artifact\n                artifact.add_file(output_path)\n                logger.info(f\"CSV file '{output_path}' added to WandB artifact qa_movie_df.\")\n    \n                # Save the artifact\n                artifact.save()\n                logger.info(f\"Artifact qa_movie_df  logged to WandB successfully.\")\n\n        except Exception as e:\n            logger.error(f\"Error saving DataFrame or uploading to WandB: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:38:13.284826Z","iopub.execute_input":"2024-10-16T21:38:13.285597Z","iopub.status.idle":"2024-10-16T21:38:13.328829Z","shell.execute_reply.started":"2024-10-16T21:38:13.285545Z","shell.execute_reply":"2024-10-16T21:38:13.327530Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from hydra.core.global_hydra import GlobalHydra\nclass DataLoaderFactory:\n    \"\"\"\n    Factory class to create DataLoader instances.\n    \"\"\"\n    @staticmethod\n    def get_data_loader(dataset_type,config, **kwargs):\n        if dataset_type == 'cornell':\n            return CornellDataLoader(config, **kwargs)\n        elif dataset_type == 'movieqa':\n            return MovieQADataLoader(config, **kwargs)\n        else:\n            raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n\n# Example Usage for Both Cornell and MovieQA DataLoader\nif __name__ == \"__main__\":\n    # Check if Hydra is already initialized and clear it if necessary\n    if GlobalHydra.instance().is_initialized():\n        GlobalHydra.instance().clear()\n    # Load .env file   \n    load_dotenv('/kaggle/input/config/.env')\n    \n    # Manually initialize Hydra in Jupyter\n    hydra.initialize(config_path=\"../input/config\", version_base=\"1.1\")  # If you're using a specific path, set config_path=<path>\n\n    # Manually compose the config from the configuration file\n    cfg = hydra.compose(config_name=\"config\")  \n    \n    wandb.login(key = cfg.wandb.api_token)\n    run = wandb.init(project= cfg.wandb.project_name, entity=cfg.wandb.entity)\n    login(cfg.hf.api_token)\n    api = HfApi()\n    # Download the entire repository (including folders)\n    local_dir = snapshot_download(repo_id = cfg.hf.repo_id, repo_type=\"dataset\", allow_patterns= cfg.hf.allow_patterns, local_dir = '/kaggle/working/')\n\n    logger.info(f\"Downloaded repository to {local_dir}\")\n    # Define dataset type (choose 'cornell' or 'movieqa')\n    dataset_type = 'cornell'  # Change to 'movieqa' for MovieQA dataset\n\n    if dataset_type == 'cornell':\n        # Parameters for Cornell Dataset\n        cornell_config_dict = OmegaConf.to_object(cfg.cornell_dataset)\n        cornell_config = CornellDatasetConfig(**cornell_config_dict)\n    \n#         column_names_list = [\n#             cfg.cornell_dataset.columns.movie_conversations,\n#             cfg.cornell_dataset.columns.movie_lines,\n#             cfg.cornell_dataset.columns.movie_titles_metadata,\n#             cfg.cornell_dataset.columns.movie_characters_metadata,\n#             cfg.cornell_dataset.columns.imdb_movie_detailed\n#         ]\n      \n#         folder_path = \"/kaggle/working/cornell\"\n#         #!wget https://huggingface.co/datasets/niloufarna/MovieChat/resolve/main/movie_detailed.csv\n        \n#         data_path_list = ['movie_conversations.txt', 'movie_lines.txt', 'movie_titles_metadata.txt', 'movie_characters_metadata.txt', 'movie_detailed.csv']\n#         column_names_list = [\n#             ['character_id1' ,'character_id2', 'movie_id','line_id_list'],\n#             ['character_id1','movie_id','character_name','utterance'],\n#             ['movie_id', 'movie_name', 'release_year', 'imdb_rating', 'num_imdb_votes', 'genre'],\n#             ['character_id','character_name','movie_id','movie_name','gender','position'],\n#             [\"title\", \"year\", \"kind\", \"cover_url\", \"original_title\", \"localized_title\", \n#             \"genres\", \"runtimes\", \"countries\", \"language_codes\", \"rating\", \"votes\", \n#             \"imdbID\", \"plot_outline\", \"languages\", \"director\", \"writer\", \"cast\", \"box_office\", \"plot\", \"synopsis\"]\n#         ]\n#         output_path = \"cornell_movie_data.json\"\n        \n        # Initialize DataLoader via factory\n        loader = DataLoaderFactory.get_data_loader(\n            dataset_type='cornell',\n            config = cornell_config\n#             folder_path = cfg.cornell_dataset.paths.folder_path,\n#             data_path_list = list(cfg.cornell_dataset.paths.data_path_list),\n#             column_names_list=column_names_list,\n#             output_path = cfg.cornell_dataset.paths.output_path,\n#             sample_fraction=cfg.cornell_dataset.sample_fraction,\n#             train_ratio=cfg.cornell_dataset.train_ratio, \n#             test_ratio = cfg.cornell_dataset.test_ratio,\n#             generate_new_questions = cfg.cornell_dataset.generate_new_questions\n        )\n\n    elif dataset_type == 'movieqa':\n        \n        # Parameters for MovieQA Dataset\n        movie_qa_config_dict = OmegaConf.to_object(cfg.movieqa_dataset)\n        movieqa_config = MovieQADatasetConfig(**movieqa_config_dict)\n        #columns_list = [cfg.movieqa_dataset.columns.imdb_movie_details_movieqa]\n        \n#         folder_path = \"/kaggle/working/movieqa\"\n#         data_path_list = [\"qa.json\", \"movies.json\", \"imdb_movie_details_movieqa.csv\"]\n#         output_path = \"movieqa_movie_data.json\"\n#         columns_list = [['movie_name', 'plot_outline']]\n        # Initialize DataLoader via factory\n        loader = DataLoaderFactory.get_data_loader(\n            dataset_type='movieqa',\n            config = movieqa_config\n#             folder_path = cfg.movieqa_dataset.paths.folder_path,\n#             data_path_list = list(cfg.movieqa_dataset.paths.data_path_list),\n#             output_path = cfg.movieqa_dataset.paths.output_path,\n#             columns_list = columns_list,\n#             train_ratio = cfg.movieqa_dataset.train_ratio, \n#             test_ratio = cfg.movieqa_dataset.test_ratio\n        )\n\n    # Load data\n    data_frames = loader.load_data()\n\n    # Preprocess data (if any preprocessing is implemented)\n    data_frames = loader.preprocess_data(data_frames)\n    \n    loader.merge_dataframes(data_frames)\n    \n    loader.train_val_test_split()\n    \n    # Convert to JSON\n    loader.convert_to_json()\n\n    # Save JSON data\n    file_path = loader.save_data()\n    dataset_type = 'movieqa'  # Change to 'movieqa' for MovieQA dataset\n\n    if dataset_type == 'cornell':\n        # Parameters for Cornell Dataset\n        cornell_config_dict = OmegaConf.to_object(cfg.cornell_dataset)\n        cornell_config = CornellDatasetConfig(**cornell_config_dict)\n    \n#         column_names_list = [\n#             cfg.cornell_dataset.columns.movie_conversations,\n#             cfg.cornell_dataset.columns.movie_lines,\n#             cfg.cornell_dataset.columns.movie_titles_metadata,\n#             cfg.cornell_dataset.columns.movie_characters_metadata,\n#             cfg.cornell_dataset.columns.imdb_movie_detailed\n#         ]\n#         folder_path = \"/kaggle/working/cornell\"\n#         #!wget https://huggingface.co/datasets/niloufarna/MovieChat/resolve/main/movie_detailed.csv\n        \n#         data_path_list = ['movie_conversations.txt', 'movie_lines.txt', 'movie_titles_metadata.txt', 'movie_characters_metadata.txt', 'movie_detailed.csv']\n#         column_names_list = [\n#             ['character_id1' ,'character_id2', 'movie_id','line_id_list'],\n#             ['character_id1','movie_id','character_name','utterance'],\n#             ['movie_id', 'movie_name', 'release_year', 'imdb_rating', 'num_imdb_votes', 'genre'],\n#             ['character_id','character_name','movie_id','movie_name','gender','position'],\n#             [\"title\", \"year\", \"kind\", \"cover_url\", \"original_title\", \"localized_title\", \n#             \"genres\", \"runtimes\", \"countries\", \"language_codes\", \"rating\", \"votes\", \n#             \"imdbID\", \"plot_outline\", \"languages\", \"director\", \"writer\", \"cast\", \"box_office\", \"plot\", \"synopsis\"]\n#         ]\n#         output_path = \"cornell_movie_data.json\"\n        \n        # Initialize DataLoader via factory\n        loader = DataLoaderFactory.get_data_loader(\n            dataset_type='cornell',\n            config = cornell_config\n#             folder_path = cfg.cornell_dataset.paths.folder_path,\n#             data_path_list = list(cfg.cornell_dataset.paths.data_path_list),\n#             column_names_list=column_names_list,\n#             output_path = cfg.cornell_dataset.paths.output_path,\n#             sample_fraction=cfg.cornell_dataset.sample_fraction,\n#             train_ratio=cfg.cornell_dataset.train_ratio, \n#             test_ratio = cfg.cornell_dataset.test_ratio,\n#             generate_new_questions = cfg.cornell_dataset.generate_new_questions\n        )\n\n    elif dataset_type == 'movieqa':\n        movieqa_config_dict = OmegaConf.to_object(cfg.movieqa_dataset)\n        movieqa_config = MovieQADatasetConfig(**movieqa_config_dict)\n        \n        # Initialize DataLoader via factory\n        loader = DataLoaderFactory.get_data_loader(\n            dataset_type='movieqa',\n            config = movieqa_config\n#             folder_path = cfg.movieqa_dataset.paths.folder_path,\n#             data_path_list = list(cfg.movieqa_dataset.paths.data_path_list),\n#             output_path = cfg.movieqa_dataset.paths.output_path,\n#             columns_list = columns_list,\n#             train_ratio = cfg.movieqa_dataset.train_ratio, \n#             test_ratio = cfg.movieqa_dataset.test_ratio\n        )\n\n    # Load data\n    data_frames = loader.load_data()\n\n    # Preprocess data (if any preprocessing is implemented)\n    data_frames = loader.preprocess_data(data_frames)\n\n    loader.merge_dataframes(data_frames)\n\n    loader.train_val_test_split()\n\n    # Convert to JSON\n    loader.convert_to_json()\n\n    # Save JSON data\n    file_path2 = loader.save_data()\n\n    combined_data = BaseDataLoader.concat_json_files(file_path, file_path2)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:38:13.330628Z","iopub.execute_input":"2024-10-16T21:38:13.331036Z","iopub.status.idle":"2024-10-16T21:39:11.821954Z","shell.execute_reply.started":"2024-10-16T21:38:13.330997Z","shell.execute_reply":"2024-10-16T21:39:11.805646Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniloufarcolab6\u001b[0m (\u001b[33mniloufarcolab6-n\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241016_213816-uixht4kj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/niloufarcolab6-n/Moviechat/runs/uixht4kj' target=\"_blank\">genial-wind-86</a></strong> to <a href='https://wandb.ai/niloufarcolab6-n/Moviechat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/niloufarcolab6-n/Moviechat' target=\"_blank\">https://wandb.ai/niloufarcolab6-n/Moviechat</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/niloufarcolab6-n/Moviechat/runs/uixht4kj' target=\"_blank\">https://wandb.ai/niloufarcolab6-n/Moviechat/runs/uixht4kj</a>"},"metadata":{}},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb70d0cf078340178369a79f813c23ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"movieqa/movies.json:   0%|          | 0.00/123k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da0527e9c59540a6ac85da7717b55fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"movie_lines.txt:   0%|          | 0.00/34.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f306123ef06a467d90d889be1e0f8bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cornell/movie_conversations.txt:   0%|          | 0.00/6.76M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62186204a2494202a10d686bf558fd5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cornell/imdb_cornell_movie_dataset.csv:   0%|          | 0.00/6.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b58b52950f9f4389a28698c4529502bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"movieqa/qa.json:   0%|          | 0.00/8.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"800b3c6161994c35ab0db536cda5728e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cornell/movie_titles_metadata.txt:   0%|          | 0.00/67.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6086a22d0bf46d7889070a82148788e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cornell/movie_characters_metadata.txt:   0%|          | 0.00/706k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba326e28b7449899f90e8b8b196677c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"movieqa/imdb_movie_details_movieqa.csv:   0%|          | 0.00/5.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f572e48a2f444c918ec83b3d1b887204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:19,671:INFO:Downloaded repository to /kaggle/working'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:22,498:INFO:Data loaded successfully.'"},"metadata":{}},{"name":"stdout","text":"preprocess imdb_details_df Index(['movie_name', 'plot_outline'], dtype='object')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:23,009:INFO:Data preprocessing completed successfully.'"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/2507199294.py:452: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.movie_dataset_df.loc[:, 'instruction'] = 'Continue the conversation between the characters.'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:30,667:INFO:Sampled 123 data successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:30,731:INFO:Cornell dataset size before pruning: 83532 rows'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:30,801:INFO:Cornell dataset size after pruning: 82231 rows'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\"2024-10-16 21:38:35,312:INFO:DataFrame saved as CSV file at './cornell/movie_dataset.csv'.\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\"2024-10-16 21:38:35,953:INFO:CSV file './cornell/movie_dataset.csv' added to WandB artifact cornell_movie_df.\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:36,104:INFO:Artifact cornell_movie_df logged to WandB successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:36,363:WARNING:Stratified split failed: The test_size = 83 should be greater or equal to the number of classes = 580. Falling back to non-stratified split.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:36,390:INFO:Training set size: 57561'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:36,392:INFO:Validation set size: 83'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:36,394:INFO:Test set size: 24587'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:42,496:INFO:Data converted to JSON format successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:47,290:INFO:Data saved to ./cornell/cornell_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\"2024-10-16 21:38:48,038:INFO:JSON file 'cornell_movie_data.json' uploaded to WandB successfully.\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cornell_movie_data.json:   0%|          | 0.00/119M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f86447dd1fa4e44b948cd685932cf14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:54,694:INFO:File cornell_movie_data.json  logged to Huggingface successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:55,177:INFO:Q&A JSON file loaded successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:55,187:INFO:Q&A JSON file loaded successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:55,217:INFO:Q&A CSV file loaded successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:55,220:INFO:Data loaded successfully.'"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/2765503092.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  qa_df['response'] = qa_df.apply(lambda row: row['answers'][int(row['correct_index'])], axis=1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\"2024-10-16 21:38:56,039:INFO:DataFrame saved as CSV file at './movieqa/qa_movie_df.csv'.\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\"2024-10-16 21:38:56,096:INFO:CSV file './movieqa/qa_movie_df.csv' added to WandB artifact qa_movie_df.\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:56,255:INFO:Artifact qa_movie_df  logged to WandB successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:56,276:WARNING:Stratified split failed: The test_size = 7 should be greater or equal to the number of classes = 81. Falling back to non-stratified split.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:56,283:INFO:Training set size: 3684'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:56,285:INFO:Validation set size: 7'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:56,288:INFO:Test set size: 1573'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:56,781:INFO:Q&A Movie dataset saved to movieqa_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:38:59,857:INFO:Artifact movieqa_movie_data.json  logged to Huggingface successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:39:00,918:INFO:Successfully loaded data from ./cornell/cornell_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:39:00,966:INFO:Successfully loaded data from ./movieqa/movieqa_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:39:00,969:INFO:Successfully concatenated data from ./cornell/cornell_movie_data.json and ./movieqa/movieqa_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"combined_movie_dataset.json:   0%|          | 0.00/125M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d9b25759f2e476aa0fdbd7d0c26df85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:39:09,995:ERROR:An unexpected error occurred: Bad request for commit endpoint:\\n[31m------------------------------------------------------------------------- Unexpected internal error hook: check-file-count. (Request ID: Root=1-6710327d-23253f3d6ae1d09b2820e0a1;b10bb3ee-f60a-47e1-8292-56ad82082ec8) ------------------------------------------------------------------------- [0m\\n\\x1b[31m-------------------------------------------------------------------------\\nUnexpected internal error hook: check-file-count. (Request ID: Root=1-6710327d-23253f3d6ae1d09b2820e0a1;b10bb3ee-f60a-47e1-8292-56ad82082ec8)\\n-------------------------------------------------------------------------\\x1b[0m'"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/niloufarna/MovieChat/commit/main","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 195\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Save JSON data\u001b[39;00m\n\u001b[1;32m    193\u001b[0m file_path2 \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39msave_data()\n\u001b[0;32m--> 195\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mBaseDataLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path2\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 86\u001b[0m, in \u001b[0;36mBaseDataLoader.concat_json_files\u001b[0;34m(file_path1, file_path2)\u001b[0m\n\u001b[1;32m     84\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, file, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mhf\u001b[38;5;241m.\u001b[39mcombined_dataset\u001b[38;5;241m.\u001b[39mto_hf:\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# commit_message=cfg.hf.combined_dataset.commit_message,\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# commit_description=cfg.hf.combined_dataset.commit_description\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mhf\u001b[38;5;241m.\u001b[39mcombined_dataset\u001b[38;5;241m.\u001b[39mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  logged to Huggingface successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mwandb\u001b[38;5;241m.\u001b[39mcombined_dataset\u001b[38;5;241m.\u001b[39mto_wandb:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1485\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4653\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4645\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4646\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4647\u001b[0m )\n\u001b[1;32m   4648\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   4649\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   4650\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[1;32m   4651\u001b[0m )\n\u001b[0;32m-> 4653\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4656\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4663\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4666\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[38;5;241m.\u001b[39mpr_url), safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1485\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3995\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   3993\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3994\u001b[0m     commit_resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39mcommit_url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m-> 3995\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3996\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3997\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n","\u001b[0;31mBadRequestError\u001b[0m: Bad request for commit endpoint:\n[31m------------------------------------------------------------------------- Unexpected internal error hook: check-file-count. (Request ID: Root=1-6710327d-23253f3d6ae1d09b2820e0a1;b10bb3ee-f60a-47e1-8292-56ad82082ec8) ------------------------------------------------------------------------- [0m\n\u001b[31m-------------------------------------------------------------------------\nUnexpected internal error hook: check-file-count. (Request ID: Root=1-6710327d-23253f3d6ae1d09b2820e0a1;b10bb3ee-f60a-47e1-8292-56ad82082ec8)\n-------------------------------------------------------------------------\u001b[0m"],"ename":"BadRequestError","evalue":"Bad request for commit endpoint:\n[31m------------------------------------------------------------------------- Unexpected internal error hook: check-file-count. (Request ID: Root=1-6710327d-23253f3d6ae1d09b2820e0a1;b10bb3ee-f60a-47e1-8292-56ad82082ec8) ------------------------------------------------------------------------- [0m\n\u001b[31m-------------------------------------------------------------------------\nUnexpected internal error hook: check-file-count. (Request ID: Root=1-6710327d-23253f3d6ae1d09b2820e0a1;b10bb3ee-f60a-47e1-8292-56ad82082ec8)\n-------------------------------------------------------------------------\u001b[0m","output_type":"error"}]},{"cell_type":"code","source":"# with open(\"/kaggle/working/cornell_movie_data.json\", \"r\") as file1:\n#     data1 = json.load(file1)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:01:44.102249Z","iopub.execute_input":"2024-10-16T21:01:44.102731Z","iopub.status.idle":"2024-10-16T21:01:44.109093Z","shell.execute_reply.started":"2024-10-16T21:01:44.102691Z","shell.execute_reply":"2024-10-16T21:01:44.107909Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"combined_data = BaseDataLoader.concat_json_files(file_path, file_path2)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T21:50:16.190735Z","iopub.execute_input":"2024-10-16T21:50:16.191225Z","iopub.status.idle":"2024-10-16T21:50:24.227512Z","shell.execute_reply.started":"2024-10-16T21:50:16.191180Z","shell.execute_reply":"2024-10-16T21:50:24.225922Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:50:17,412:INFO:Successfully loaded data from ./cornell/cornell_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:50:17,470:INFO:Successfully loaded data from ./movieqa/movieqa_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:50:17,473:INFO:Successfully concatenated data from ./cornell/cornell_movie_data.json and ./movieqa/movieqa_movie_data.json'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:50:23,296:INFO:File combined_movie_dataset.json  logged to Huggingface successfully.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'2024-10-16 21:50:24,145:INFO:File combined_movie_dataset.json  logged to WandB successfully.'"},"metadata":{}}]},{"cell_type":"code","source":"# with open(\"/kaggle/working/movieqa_movie_data.json\", \"r\") as file2:\n#     data2 = json.load(file2)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T19:04:15.812201Z","iopub.execute_input":"2024-10-16T19:04:15.812713Z","iopub.status.idle":"2024-10-16T19:04:15.819644Z","shell.execute_reply.started":"2024-10-16T19:04:15.812669Z","shell.execute_reply":"2024-10-16T19:04:15.818334Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# combined_data = data1 + data2","metadata":{"execution":{"iopub.status.busy":"2024-10-03T15:43:18.276724Z","iopub.status.idle":"2024-10-03T15:43:18.277211Z","shell.execute_reply.started":"2024-10-03T15:43:18.276976Z","shell.execute_reply":"2024-10-03T15:43:18.276998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/movie_data.json', 'w', encoding='utf-8') as output_file:\n    json.dump(combined_data, output_file, ensure_ascii=False, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T15:43:18.279648Z","iopub.status.idle":"2024-10-03T15:43:18.280169Z","shell.execute_reply.started":"2024-10-03T15:43:18.279916Z","shell.execute_reply":"2024-10-03T15:43:18.279940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(combined_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T15:43:18.283980Z","iopub.status.idle":"2024-10-03T15:43:18.286707Z","shell.execute_reply.started":"2024-10-03T15:43:18.286398Z","shell.execute_reply":"2024-10-03T15:43:18.286441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the JSON file into pandas for inspection\nfile_path = '/kaggle/working/cornell_movie_data.json'  # Replace with the path to your JSON file\n\n# Loading the JSON file into a pandas DataFrame\ndf = pd.read_json(file_path)\n\n# Display the first few rows of the DataFrame to inspect the structure\nprint(df.head())\n\n# Display column types to check for consistency\nprint(df.dtypes)\n\n# Check for mixed types or inconsistent data in specific columns (like context or nested fields)\nfor col in df.columns:\n    print(f\"Checking column: {col}\")\n    print(df[col].apply(type).value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T15:43:18.288347Z","iopub.status.idle":"2024-10-03T15:43:18.288872Z","shell.execute_reply.started":"2024-10-03T15:43:18.288635Z","shell.execute_reply":"2024-10-03T15:43:18.288659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\n# Download the entire repository (including folders)\nlocal_dir = snapshot_download(repo_id=\"niloufarna/MovieChat\", repo_type=\"dataset\", allow_patterns=[\"cornell/*\",\"imdb/*\", \"movieqa/*\"], local_dir = '/kaggle/working/')\n\nprint(f\"Downloaded repository to {local_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T15:43:18.291670Z","iopub.status.idle":"2024-10-03T15:43:18.292224Z","shell.execute_reply.started":"2024-10-03T15:43:18.291969Z","shell.execute_reply":"2024-10-03T15:43:18.291995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('/kaggle/working/movie_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:51:14.638102Z","iopub.execute_input":"2024-10-03T23:51:14.638599Z","iopub.status.idle":"2024-10-03T23:51:15.187780Z","shell.execute_reply.started":"2024-10-03T23:51:14.638553Z","shell.execute_reply":"2024-10-03T23:51:15.186618Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       character_names                    movie_name  release_year  \\\n0      Bianca, Cameron    10 things i hate about you          1999   \n1      Bianca, Cameron    10 things i hate about you          1999   \n2      Bianca, Cameron    10 things i hate about you          1999   \n3      Bianca, Cameron    10 things i hate about you          1999   \n4      Bianca, Cameron    10 things i hate about you          1999   \n...                ...                           ...           ...   \n83651      Adam, Billy  mighty morphin power rangers          1994   \n83652  Camille, Monica             love & basketball          2000   \n83653  Camille, Monica             love & basketball          2000   \n83654  Camille, Monica             love & basketball          2000   \n83655  Camille, Monica             love & basketball          2000   \n\n       imdb_rating  num_imdb_votes                  genre  \\\n0              6.9           62847        comedy, romance   \n1              6.9           62847        comedy, romance   \n2              6.9           62847        comedy, romance   \n3              6.9           62847        comedy, romance   \n4              6.9           62847        comedy, romance   \n...            ...             ...                    ...   \n83651          8.2              35         action, family   \n83652          6.7            5907  drama, romance, sport   \n83653          6.7            5907  drama, romance, sport   \n83654          6.7            5907  drama, romance, sport   \n83655          6.7            5907  drama, romance, sport   \n\n                                               utterance  \\\n0      Bianca: Can we make this quick?  Roxanne Korri...   \n1      Bianca: You're asking me out.  That's so cute....   \n2      Bianca: No, no, it's my fault -- we didn't hav...   \n3      Cameron: Why?\\n Bianca: Unsolved mystery.  She...   \n4      Bianca: Gosh, if only we could find Kat a boyf...   \n...                                                  ...   \n83651  How many votes does mighty morphin power range...   \n83652            What type of film is love & basketball?   \n83653       In what year was love & basketball released?   \n83654  What rating did love & basketball receive on I...   \n83655   How many people rated love & basketball on IMDb?   \n\n                                                response  plot_outline  \\\n0      Cameron: Okay... then how 'bout we try out som...           NaN   \n1                                    Cameron: Forget it.           NaN   \n2      Cameron: Seems like she could get a date easy ...           NaN   \n3                               Cameron: That's a shame.           NaN   \n4                     Cameron: Let me see what I can do.           NaN   \n...                                                  ...           ...   \n83651                                                 35           NaN   \n83652                              drama, romance, sport           NaN   \n83653                                               2000           NaN   \n83654                                                6.7           NaN   \n83655                                               5907           NaN   \n\n                                             instruction  \n0      Continue the conversation between the characters.  \n1      Continue the conversation between the characters.  \n2      Continue the conversation between the characters.  \n3      Continue the conversation between the characters.  \n4      Continue the conversation between the characters.  \n...                                                  ...  \n83651                     Answer the following question:  \n83652                     Answer the following question:  \n83653                     Answer the following question:  \n83654                     Answer the following question:  \n83655                     Answer the following question:  \n\n[83656 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>character_names</th>\n      <th>movie_name</th>\n      <th>release_year</th>\n      <th>imdb_rating</th>\n      <th>num_imdb_votes</th>\n      <th>genre</th>\n      <th>utterance</th>\n      <th>response</th>\n      <th>plot_outline</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bianca, Cameron</td>\n      <td>10 things i hate about you</td>\n      <td>1999</td>\n      <td>6.9</td>\n      <td>62847</td>\n      <td>comedy, romance</td>\n      <td>Bianca: Can we make this quick?  Roxanne Korri...</td>\n      <td>Cameron: Okay... then how 'bout we try out som...</td>\n      <td>NaN</td>\n      <td>Continue the conversation between the characters.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bianca, Cameron</td>\n      <td>10 things i hate about you</td>\n      <td>1999</td>\n      <td>6.9</td>\n      <td>62847</td>\n      <td>comedy, romance</td>\n      <td>Bianca: You're asking me out.  That's so cute....</td>\n      <td>Cameron: Forget it.</td>\n      <td>NaN</td>\n      <td>Continue the conversation between the characters.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bianca, Cameron</td>\n      <td>10 things i hate about you</td>\n      <td>1999</td>\n      <td>6.9</td>\n      <td>62847</td>\n      <td>comedy, romance</td>\n      <td>Bianca: No, no, it's my fault -- we didn't hav...</td>\n      <td>Cameron: Seems like she could get a date easy ...</td>\n      <td>NaN</td>\n      <td>Continue the conversation between the characters.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bianca, Cameron</td>\n      <td>10 things i hate about you</td>\n      <td>1999</td>\n      <td>6.9</td>\n      <td>62847</td>\n      <td>comedy, romance</td>\n      <td>Cameron: Why?\\n Bianca: Unsolved mystery.  She...</td>\n      <td>Cameron: That's a shame.</td>\n      <td>NaN</td>\n      <td>Continue the conversation between the characters.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bianca, Cameron</td>\n      <td>10 things i hate about you</td>\n      <td>1999</td>\n      <td>6.9</td>\n      <td>62847</td>\n      <td>comedy, romance</td>\n      <td>Bianca: Gosh, if only we could find Kat a boyf...</td>\n      <td>Cameron: Let me see what I can do.</td>\n      <td>NaN</td>\n      <td>Continue the conversation between the characters.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>83651</th>\n      <td>Adam, Billy</td>\n      <td>mighty morphin power rangers</td>\n      <td>1994</td>\n      <td>8.2</td>\n      <td>35</td>\n      <td>action, family</td>\n      <td>How many votes does mighty morphin power range...</td>\n      <td>35</td>\n      <td>NaN</td>\n      <td>Answer the following question:</td>\n    </tr>\n    <tr>\n      <th>83652</th>\n      <td>Camille, Monica</td>\n      <td>love &amp; basketball</td>\n      <td>2000</td>\n      <td>6.7</td>\n      <td>5907</td>\n      <td>drama, romance, sport</td>\n      <td>What type of film is love &amp; basketball?</td>\n      <td>drama, romance, sport</td>\n      <td>NaN</td>\n      <td>Answer the following question:</td>\n    </tr>\n    <tr>\n      <th>83653</th>\n      <td>Camille, Monica</td>\n      <td>love &amp; basketball</td>\n      <td>2000</td>\n      <td>6.7</td>\n      <td>5907</td>\n      <td>drama, romance, sport</td>\n      <td>In what year was love &amp; basketball released?</td>\n      <td>2000</td>\n      <td>NaN</td>\n      <td>Answer the following question:</td>\n    </tr>\n    <tr>\n      <th>83654</th>\n      <td>Camille, Monica</td>\n      <td>love &amp; basketball</td>\n      <td>2000</td>\n      <td>6.7</td>\n      <td>5907</td>\n      <td>drama, romance, sport</td>\n      <td>What rating did love &amp; basketball receive on I...</td>\n      <td>6.7</td>\n      <td>NaN</td>\n      <td>Answer the following question:</td>\n    </tr>\n    <tr>\n      <th>83655</th>\n      <td>Camille, Monica</td>\n      <td>love &amp; basketball</td>\n      <td>2000</td>\n      <td>6.7</td>\n      <td>5907</td>\n      <td>drama, romance, sport</td>\n      <td>How many people rated love &amp; basketball on IMDb?</td>\n      <td>5907</td>\n      <td>NaN</td>\n      <td>Answer the following question:</td>\n    </tr>\n  </tbody>\n</table>\n<p>83656 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nfor dirpath, dirname, filenames in os.walk('/kaggle/working'):\n    print(dirpath)\n    print(dirname)\n    print(filenames)\n    print(\"$$$$$$$$$$$$$$\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:10:31.655291Z","iopub.execute_input":"2024-10-03T23:10:31.655793Z","iopub.status.idle":"2024-10-03T23:10:31.684590Z","shell.execute_reply.started":"2024-10-03T23:10:31.655746Z","shell.execute_reply":"2024-10-03T23:10:31.683406Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working\n['imdb', 'movieqa', 'wandb', '.cache', '.virtual_documents', 'cornell']\n['movie_dataset.csv']\n$$$$$$$$$$$$$$\n/kaggle/working/imdb\n[]\n['imdb_movie_details_cornell.csv', 'imdb_movie_details_movieqa.csv']\n$$$$$$$$$$$$$$\n/kaggle/working/movieqa\n[]\n['qa.json', 'imdb_movie_details_movieqa.csv', 'movies.json']\n$$$$$$$$$$$$$$\n/kaggle/working/wandb\n['run-20241003_225825-af981qko', 'latest-run']\n['debug.log', 'debug-internal.log']\n$$$$$$$$$$$$$$\n/kaggle/working/wandb/run-20241003_225825-af981qko\n['tmp', 'logs', 'files']\n['run-af981qko.wandb']\n$$$$$$$$$$$$$$\n/kaggle/working/wandb/run-20241003_225825-af981qko/tmp\n['code']\n[]\n$$$$$$$$$$$$$$\n/kaggle/working/wandb/run-20241003_225825-af981qko/tmp/code\n[]\n[]\n$$$$$$$$$$$$$$\n/kaggle/working/wandb/run-20241003_225825-af981qko/logs\n[]\n['debug.log', 'debug-core.log', 'debug-internal.log']\n$$$$$$$$$$$$$$\n/kaggle/working/wandb/run-20241003_225825-af981qko/files\n[]\n['output.log', 'wandb-metadata.json', 'requirements.txt']\n$$$$$$$$$$$$$$\n/kaggle/working/.cache\n['huggingface']\n[]\n$$$$$$$$$$$$$$\n/kaggle/working/.cache/huggingface\n['download']\n['.gitignore']\n$$$$$$$$$$$$$$\n/kaggle/working/.cache/huggingface/download\n['imdb', 'movieqa', 'cornell']\n[]\n$$$$$$$$$$$$$$\n/kaggle/working/.cache/huggingface/download/imdb\n[]\n['imdb_movie_details_movieqa.csv.lock', 'imdb_movie_details_cornell.csv.metadata', 'imdb_movie_details_cornell.csv.lock', 'imdb_movie_details_movieqa.csv.metadata']\n$$$$$$$$$$$$$$\n/kaggle/working/.cache/huggingface/download/movieqa\n[]\n['qa.json.lock', 'imdb_movie_details_movieqa.csv.lock', 'movies.json.lock', 'movies.json.metadata', 'qa.json.metadata', 'imdb_movie_details_movieqa.csv.metadata']\n$$$$$$$$$$$$$$\n/kaggle/working/.cache/huggingface/download/cornell\n[]\n['movie_conversations.txt.lock', 'movie_titles_metadata.txt.lock', 'movie_titles_metadata.txt.metadata', 'imdb_movie_details_cornell.csv.metadata', 'imdb_movie_details_cornell.csv.lock', 'movie_characters_metadata.txt.lock', 'movie_lines.txt.lock', 'movie_conversations.txt.metadata', 'movie_characters_metadata.txt.metadata', 'movie_lines.txt.metadata']\n$$$$$$$$$$$$$$\n/kaggle/working/.virtual_documents\n[]\n[]\n$$$$$$$$$$$$$$\n/kaggle/working/cornell\n[]\n['movie_lines.txt', 'imdb_movie_details_cornell.csv', 'movie_conversations.txt', 'movie_titles_metadata.txt', 'movie_characters_metadata.txt', 'cornell_movie_data.json']\n$$$$$$$$$$$$$$\n","output_type":"stream"}]}]}
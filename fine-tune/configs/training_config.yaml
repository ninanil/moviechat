per_device_train_batch_size: 4
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8
warmup_steps: 191
eval_strategy: "steps"
eval_steps: 3
save_steps: 3
save_total_limit: 3
num_train_epochs: 1
learning_rate: 1e-4
fp16: true
logging_steps: 3
optim: "adamw_8bit"
weight_decay: 0.01
lr_scheduler_type: "cosine_with_restarts"
seed: 3407
output_dir: "outputs"
report_to: "mlflow"
